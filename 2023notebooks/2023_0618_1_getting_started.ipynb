{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/2015corona/blob/master/2023notebooks/2023_0618_1_getting_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "date: 2023_0618\n",
        "---\n",
        "\n",
        "stablebaselines3\n",
        "from <https://araffin.github.io/post/sb3/>"
      ],
      "metadata": {
        "id": "8i5xrt688kId"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Stable Baselines3 Tutorial - Getting Started\n",
        "\n",
        "Github repo: https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3/\n",
        "\n",
        "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentation: https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        "SB3-Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
        "\n",
        "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) は，Stable Baselines3 を用いた強化学習 (RL) のための訓練フレームワーク。\n",
        "<!-- [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a training framework for Reinforcement Learning (RL), using Stable Baselines3.-->\n",
        "\n",
        "訓練，エージェントの評価，ハイパーパラメータの調整，結果のプロット，ビデオ撮影のためのスクリプトが用意されている。\n",
        "<!-- It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos. -->\n",
        "\n",
        "\n",
        "## はじめに <!-- ## Introduction-->\n",
        "\n",
        "このノートブックでは，stable baselines ライブラリの基本的な使い方，つまり RL モデルの作成方法，訓練方法，評価方法を学ぶ。\n",
        "すべてのアルゴリズムは同じインターフェイスを共有しているため，あるアルゴリズムから別のアルゴリズムへの切り替えがいかに簡単であるかを確認することができる。\n",
        "<!--In this notebook, you will learn the basics for using stable baselines library: how to create a RL model, train it and evaluate it.\n",
        "Because all algorithms share the same interface, we will see how simple it is to switch from one algorithm to another. -->\n",
        "\n",
        "\n",
        "##  Pip を用いた依存関係と安定した Baselines3 のインストール <!-- ## Install Dependencies and Stable Baselines3 Using Pip-->\n",
        "\n",
        "完全な依存関係のリストは [README](https://github.com/DLR-RM/stable-baselines3) に記載されている。\n",
        "<!-- List of full dependencies can be found in the [README](https://github.com/DLR-RM/stable-baselines3). -->\n",
        "\n",
        "\n",
        "Zsh などを使っている M1 Mac などでは，シングルクォートが必須\n",
        "```\n",
        "pip install 'stable-baselines3[extra]'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "isColab =  'google.colab' in str(get_ipython())\n",
        "if isColab:\n",
        "    !pip install jupyter-black"
      ],
      "metadata": {
        "id": "JjFyFoUpGIkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PSqxDQ-48T41"
      },
      "outputs": [],
      "source": [
        "# for autoformatting\n",
        "%load_ext jupyter_black"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "outputs": [],
      "source": [
        "if isColab:\n",
        "    !apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "    !pip install \"stable-baselines3[extra]>=2.0.0a4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## 輸入 Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcX8hEcaUpR0"
      },
      "source": [
        "Stable-Baselines3は、[gym interface](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html)に従った環境上で動作する。\n",
        "利用可能な環境のリストは [こちら](https://gymnasium.farama.org/environments/classic_control/) で見ることができる。\n",
        "<!-- Stable-Baselines3 works on environments that follow the [gym interface](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html).\n",
        "You can find a list of available environment [here](https://gymnasium.farama.org/environments/classic_control/).-->\n",
        "\n",
        "すべてのアルゴリズムがすべての行動空間で動作するわけではない。\n",
        "詳しくはこの [総括表](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html) 参照。\n",
        "<!-- Not all algorithms can work with all action spaces, you can find more in this [recap table](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html) -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae32CtgzTG3R"
      },
      "source": [
        "最初に輸入する必要があるのは RL モデルで，どの問題で何が使えるかはドキュメントを確認せよ。\n",
        "<!-- The first thing you need to import is the RL model, check the documentation to know what you can use on which problem -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R7tKaBFrTR0a"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0_8OQbOTTNT"
      },
      "source": [
        "次に輸入する必要があるのは，(方針/価値関数の) ネットワークを作成するために使用される方針クラスである。\n",
        "コンストラクタで直接文字列を使用することができるので，このステップはオプションである：\n",
        "<!-- The next thing you need to import is the policy class that will be used to create the networks (for the policy/value functions).\n",
        "This step is optional as you can directly use strings in the constructor: -->\n",
        "\n",
        "`PPO(MlpPolicy, env)` の替わりに `PPO('MlpPolicy', env)`\n",
        "\n",
        "SAC のようないくつかのアルゴリズムは，独自の `MlpPolicy` を持つので，方針に文字列を使うことが推奨される。\n",
        "<!-- Note that some algorithms like `SAC` have their own `MlpPolicy`, that's why using string for the policy is the recommended option. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROUJr675TT01"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.ppo.policies import MlpPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## Gym env の作成とエージェントの実体化 <!-- ## Create the Gym env and instantiate the agent-->\n",
        "\n",
        "この例では，古典的な制御問題である CartPole 環境を使用する。\n",
        "<!--For this example, we will use CartPole environment, a classic control problem.-->\n",
        "\n",
        "“ポールは，摩擦のない軌道を移動するカートに，(アクチュエータなしの) ジョイントで取り付けられている。\n",
        "この系は，カートに +1 または -1 の力を加えることで制御される。\n",
        "振り子は直立した状態でスタートし，倒れないようにすることが目標となる。\n",
        "振り子が直立したままであれば，1 タイムステップごとに +1 の報酬が与えられる。\"\n",
        "<!-- \"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
        "The system is controlled by applying a force of +1 or -1 to the cart.\n",
        "The pendulum starts upright, and the goal is to prevent it from falling over.\n",
        "A reward of +1 is provided for every timestep that the pole remains upright. \" -->\n",
        "\n",
        "倒立振子 (Cartpole) 環境  <!--environment-->: [https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif\" width=\"33%\">\n",
        "</center>\n",
        "\n",
        "<!-- ![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif) -->\n",
        "\n",
        "MlpPolicy を選んだのは，倒立振子 (CartPole) 課題の観測が画像ではなく，特徴ベクトルであるためである。\n",
        "<!-- We chose the MlpPolicy because the observation of the CartPole task is a feature vector, not images.-->\n",
        "\n",
        "使用する行動の種類 (離散／連続) は，環境の行動空間から自動的に推論される。\n",
        "<!-- The type of action to use (discrete/continuous) will be automatically deduced from the environment action space -->\n",
        "\n",
        "ここでは [近位方針最適化 Proximal Policy Optimization](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) アルゴリズムを使用している。\n",
        "これは アクター・クリティック (Actor-Critic) 法の一つで，価値関数を使用して方針の勾配降下を (分散を減らすことによって) 改善するアルゴリズムである。\n",
        "<!-- Here we are using the [Proximal Policy Optimization](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) algorithm, which is an Actor-Critic method: it uses a value function to improve the policy gradient descent (by reducing the variance). -->\n",
        "\n",
        "[A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html) (複数のワーカーを持ち，探索にエントロピーボーナスを使う) と [信頼領域方針最適化 TRPO](https://stable-baselines.readthedocs.io/en/master/modules/trpo.html) (信頼領域を用い，安定性を向上させて性能の壊滅的低下を回避する) のアイデアを組み合わせたものである。\n",
        "<!-- It combines ideas from [A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html) (having multiple workers and using an entropy bonus for exploration) and [TRPO](https://stable-baselines.readthedocs.io/en/master/modules/trpo.html) (it uses a trust region to improve stability and avoid catastrophic drops in performance).-->\n",
        "\n",
        "PPO はオンポリシーアルゴリズムであり，ネットワークの更新に使用する軌跡は最新のポリシーを使用して収集されなければならないことを意味する。\n",
        "通常，[DQN](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html), [TD3](https://stable-baselines3.readthedocs.io/en/master/modules/td3.html) のようなオフポリシーアルゴリズムよりもサンプル効率が劣るが，ウォールクロック時間に関してははるかに高速である。\n",
        "<!--PPO is an on-policy algorithm, which means that the trajectories used to update the networks must be collected using the latest policy.\n",
        "It is usually less sample efficient than off-policy alorithms like [DQN](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html) or [TD3](https://stable-baselines3.readthedocs.io/en/master/modules/td3.html), but is much faster regarding wall-clock time. -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pUWGZp3i9wyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dedd8f8d-d557-4c41-85d7-78934d9c9835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "model = PPO(MlpPolicy, env, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efFdrQ7MBvl"
      },
      "source": [
        "エージェントを評価するためのヘルパー関数を作成する：\n",
        "<!-- We create a helper function to evaluate the agent: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "63M8mSKR-6Zt"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.base_class import BaseAlgorithm\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: BaseAlgorithm,\n",
        "    num_episodes: int = 100,\n",
        "    deterministic: bool = True,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    `num_eposodes` に渡って強化学習エージェントを評価する\n",
        "\n",
        "    :param model: 強化学習エージェント\n",
        "    :param env: gym 環境\n",
        "    :param num_episodes: 評価のためのエピソード回数\n",
        "    :param deterministic: 決定論的行動を用いるか否かのフラグ\n",
        "    :return: 直近 `num_episodes` の平均報酬\n",
        "    \"\"\"\n",
        "    # この関数は，単一環境でのみ動作する\n",
        "    vec_env = model.get_env()\n",
        "    obs = vec_env.reset()\n",
        "    all_episode_rewards = []\n",
        "    for _ in range(num_episodes):\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        # 注: SB3 VecEnv は自動的にリセットされる\n",
        "        # https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\n",
        "        # obs = vec_env.reset()\n",
        "        while not done:\n",
        "            # _states は LSTM 方針で有益\n",
        "            # `deterministic` は決定論的行動を行うか否か\n",
        "            action, _states = model.predict(obs, deterministic=deterministic)\n",
        "            # `action`, `rewards`, `dones` は配列\n",
        "            # vectorized env を用いているため\n",
        "            obs, reward, done, _info = vec_env.step(action)\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "        all_episode_rewards.append(sum(episode_rewards))\n",
        "\n",
        "    mean_episode_reward = np.mean(all_episode_rewards)\n",
        "    print(f\"Mean reward: {mean_episode_reward:.2f} - Num episodes: {num_episodes}\")\n",
        "\n",
        "    return mean_episode_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjEVOIY8NVeK"
      },
      "source": [
        "訓練されていないエージェントを評価してみる。\n",
        "このために，初期状態はランダムなエージェントである。\n",
        "<!-- Let's evaluate the un-trained agent, this should be a random agent. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xDHLMA6NFk95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5317571-9e20-4211-a4c3-fd133789e404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward: 9.13 - Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# 訓練前の乱数で初期化された動作主\n",
        "mean_reward_before_train = evaluate(model, num_episodes=100, deterministic=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjjPxrwkYJ2i"
      },
      "source": [
        "Stable-Baselines はすでにそのヘルパーを提供している：\n",
        "<!-- Stable-Baselines already provides you with that helper: -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8z6K9YImYJEx"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4oPTHjxyZSOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a3336bc-34b3-4648-8235-6fffd65da27d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward: 9.17 +/- 0.68\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100, warn=False)\n",
        "\n",
        "print(f\"mean_reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## エージェント (動作主) の訓練と評価"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "e4cfSXIB-pTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e03aea-8b3f-4898-e953-7ef517268c80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7e835a0bfca0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# 動作主を 10000 ステップ訓練\n",
        "model.learn(total_timesteps=10_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygl_gVmV_QP7"
      },
      "outputs": [],
      "source": [
        "# 訓練済の動作主を評価\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A00W6yY3NkHG"
      },
      "source": [
        "訓練はうまくいったようで，平均報酬は大きく伸びた！\n",
        "<!-- Apparently the training went well, the mean reward increased a lot ! -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### 動画撮影の準備 <!-- ### Prepare video recording -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "outputs": [],
      "source": [
        "# フェイク画面のセットアップ，そうしないとレンダリングに失敗する\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "def show_videos(video_path=\"\", prefix=\"\"):\n",
        "    \"\"\"\n",
        "    https://github.com/eleurent/highway-env より援用\n",
        "\n",
        "    :param video_path: (str) 動画が格納されているフォルダのパス\n",
        "    :param prefix: (str) この接頭辞のついた動画のみを表示する\n",
        "    \"\"\"\n",
        "    html = []\n",
        "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append(\n",
        "            \"\"\"<video alt=\"{}\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>\"\"\".format(\n",
        "                mp4, video_b64.decode(\"ascii\")\n",
        "            )\n",
        "        )\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRNUfulOGaF"
      },
      "source": [
        "[VecVideoRecorder](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) ラッパーを使って動画を録画する。\n",
        "このラッパーについては，次のノートブックで学ぶ。\n",
        "<!-- We will record a video using the [VecVideoRecorder](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you will learn about those wrapper in the next notebook. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Trag9dQpOIhx"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix=\"\", video_folder=\"videos/\"):\n",
        "    \"\"\"\n",
        "    :param env_id: (str)\n",
        "    :param model: (RL model)\n",
        "    :param video_length: (int)\n",
        "    :param prefix: (str)\n",
        "    :param video_folder: (str)\n",
        "    \"\"\"\n",
        "    eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "    # Start the video at step=0 and record 500 steps\n",
        "    eval_env = VecVideoRecorder(\n",
        "        eval_env,\n",
        "        video_folder=video_folder,\n",
        "        record_video_trigger=lambda step: step == 0,\n",
        "        video_length=video_length,\n",
        "        name_prefix=prefix,\n",
        "    )\n",
        "\n",
        "    obs = eval_env.reset()\n",
        "    for _ in range(video_length):\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "    # Close the video recorder\n",
        "    eval_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOObbeu5MMlR"
      },
      "source": [
        "### 学習済みエージェントの可視化\n",
        "<!-- ### Visualize trained agent -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iATu7AiyMQW2"
      },
      "outputs": [],
      "source": [
        "record_video(\"CartPole-v1\", model, video_length=500, prefix=\"ppo-cartpole\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n4i-fW3NojZ"
      },
      "outputs": [],
      "source": [
        "show_videos(\"videos\", prefix=\"ppo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y8zg4V566qD"
      },
      "source": [
        "## ボーナス：1 行で RL モデルを訓練 <!-- ## Bonus: Train a RL Model in One Line -->\n",
        "\n",
        "使用するポリシークラスが推論され，環境も自動的に作成される。\n",
        "どちらも [登録済 (registered)](https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html) であるため，動作する。\n",
        "<!-- The policy class to use will be inferred and the environment will be automatically created.\n",
        "This works because both are [registered](https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html). -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaOPfOrwWEP4"
      },
      "outputs": [],
      "source": [
        "model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrI6f5fWnzp-"
      },
      "source": [
        "## まとめ <!-- ## Conclusion-->\n",
        "\n",
        "このノートブックでは\n",
        "* stablebaselines3 を使って RL モデルを定義，訓練する方法を示した。たった 1 行のコードで可能である ;)\n",
        "\n",
        "<!-- In this notebook we have seen:\n",
        "- how to define and train a RL model using stable baselines3, it takes only one line of code ;) -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73ji3gbNDkf7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}