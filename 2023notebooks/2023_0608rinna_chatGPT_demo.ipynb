{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/2015corona/blob/master/2023notebooks/2023_0608rinna_chatGPT_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "import torch\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "\n",
        "if isColab:\n",
        "    !pip install 'transformers[torch]'\n",
        "    !pip install sentencepiece"
      ],
      "metadata": {
        "id": "AXM49OFY9R-h"
      },
      "id": "AXM49OFY9R-h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "841b65c0-6c41-4888-8706-50627313b3e1",
      "metadata": {
        "id": "841b65c0-6c41-4888-8706-50627313b3e1"
      },
      "source": [
        "[japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2)\n",
        "\n",
        "## 概要 <!-- ## Overview-->\n",
        "\n",
        "このリポジトリは，36 億個のパラメータを持つ日本語 GPT-NeoX モデルを提供する。\n",
        "このモデルは rinna/japanese-gpt-neox-3.6b をベースに，命令に従う会話エージェントとして機能するように細調整されている。\n",
        "<!-- This repository provides a Japanese GPT-NeoX model of 3.6 billion parameters. \n",
        "The model is based on rinna/japanese-gpt-neox-3.6b and has been finetuned to serve as an instruction-following conversational agent. -->\n",
        "\n",
        "このモデルは，従来の SFT モデル rinna/japanese-gpt-neox-3.6b-instruction-sft とは若干異なり，学習に使用するデータ分割が異なっている。\n",
        "<!--This model slightly differs from the previous SFT model rinna/japanese-gpt-neox-3.6b-instruction-sft, where a different data split is used for training.-->\n",
        " \n",
        "* モデルアーキテクチャ <!-- * Model architecture-->\n",
        "\n",
        "36層，2816 隠れ層サイズのtransformerベースの言語モデル。\n",
        "<!--A 36-layer, 2816-hidden-size transformer-based language model.  -->\n",
        "\n",
        "* SFT と従来の SFT の比較評価 <!-- * SFT vs. previous SFT evaluation-->\n",
        "\n",
        "本 SFT モデルと従来 SFT モデルの性能差を評価するため，100 プロンプトで ChatGPTベースの自動評価を実施した。\n",
        "<!-- We conducted ChatGPT-based automated evaluation on 100 prompts to assess the performance difference between this SFT model and the previous SFT model.  -->\n",
        "\n",
        "|this SFT vs. previous SFT| win|tie|loss|\n",
        "|:---:|:---:|:---:|:---:|\n",
        "|ChatGPT auto. evaluation|\t55%|\t0%\t|45%|\n",
        "\n",
        "* 微調整 <!--Finetuning-->\n",
        "\n",
        "微調整データは，以下のデータセットの下位集合であり，日本語に翻訳されている。 <!-- The finetuning data is the subset of the following datasets and has been translated into Japanese.-->\n",
        "\n",
        "* [人間工学的 HH RLHF データ](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n",
        "* [FLAN Instruction Tuning データ](https://github.com/google-research/FLAN)\n",
        "* [スタンフォード 人間嗜好データセット (Stanford Human Preferences Dataset)](https://huggingface.co/datasets/stanfordnlp/SHP)\n",
        "\n",
        "データは公開されない\n",
        "\n",
        "<!--* Anthropic HH RLHF data\n",
        "* FLAN Instruction Tuning data\n",
        "* Stanford Human Preferences Dataset\n",
        "\n",
        "The data will not be released. -->\n",
        "\n",
        "* Authors\n",
        "\n",
        "Tianyu Zhao and Kei Sawada\n",
        "\n",
        "###  I/O Format\n",
        "\n",
        "入力を構成するために、特別なフォーマットを採用している。 <!-- A special format has been adopted to construct inputs.-->\n",
        "\n",
        "* 入力プロンプトは，ユーザーとシステムの会話としてフォーマットされている。\n",
        "* 各入力発話は，(1) 話者 (`ユーザ` または `システム`)，(2) コロン (`:`)，(3) 空白 (` `)，(4) 発話テキスト(例: `世界で一番高い山は？`)\n",
        "* 入力プロンプトは `システム` で終了する必要があり，モデルが応答を生成することを許可する。\n",
        "* 入力と出力に含まれるすべての改行記号を `<NL>` に置き換える必要がある。\n",
        "* 入力プロンプト内の発言は全て `<NL>` で区切る。\n",
        "\n",
        "以下は，会話から入力を作成する例である。\n",
        "\n",
        "<!-- * An input prompt is formatted as a conversation between ユーザー and システム.\n",
        "* Each input utterance consists of (1) its speaker (\"ユーザー\" or \"システム\"), (2) a colon (\":\"), (3) a whitespace (\" \"), and (4) utterance text (e.g. \"世界で一番高い山は？\").\n",
        "* The input prompt should be ended with \"システム: \" to acknowledge the model to generate a response.\n",
        "* Since the model's tokenizer does not recognize \"\\n\", a special newline symbol \"<NL>\" is used instead.\n",
        "* All the newlines in input and output utterances should be replaced with \"<NL>\".\n",
        "* All the utterances in the input prompt should be separated by \"<NL>\".\n",
        "Following is an example to construct an input from a conversation. -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d11df4f9-89b7-429e-b511-fd474a0fbb89",
      "metadata": {
        "id": "d11df4f9-89b7-429e-b511-fd474a0fbb89"
      },
      "outputs": [],
      "source": [
        "prompt = [\n",
        "    {\n",
        "        \"speaker\": \"ユーザー\",\n",
        "        \"text\": \"コンタクトレンズを慣れるにはどうすればよいですか？\"\n",
        "    },\n",
        "    {\n",
        "        \"speaker\": \"システム\",\n",
        "        \"text\": \"これについて具体的に説明していただけますか？何が難しいのでしょうか？\"\n",
        "    },\n",
        "    {\n",
        "        \"speaker\": \"ユーザー\",\n",
        "        \"text\": \"目が痛いのです。\"\n",
        "    },\n",
        "    {\n",
        "        \"speaker\": \"システム\",\n",
        "        \"text\": \"分かりました、コンタクトレンズをつけると目がかゆくなるということですね。思った以上にレンズを外す必要があるでしょうか？\"\n",
        "    },\n",
        "    {\n",
        "        \"speaker\": \"ユーザー\",\n",
        "        \"text\": \"いえ、レンズは外しませんが、目が赤くなるんです。\"\n",
        "    }\n",
        "]\n",
        "prompt = [\n",
        "    f\"{uttr['speaker']}: {uttr['text']}\"\n",
        "    for uttr in prompt\n",
        "]\n",
        "prompt = \"<NL>\".join(prompt)\n",
        "prompt = (\n",
        "    prompt\n",
        "    + \"<NL>\"\n",
        "    + \"システム: \"\n",
        ")\n",
        "print(prompt)\n",
        "# \"ユーザー: コンタクトレンズを慣れるにはどうすればよいですか？<NL>システム: これについて具体的に説明していただけますか？何が難しいのでしょうか？<NL>ユーザー: 目が痛いのです。<NL>システム: 分かりました、コンタクトレンズをつけると目がかゆくなるということですね。思った以上にレンズを外す必要があるでしょうか？<NL>ユーザー: いえ、レンズは外しませんが、目が赤くなるんです。<NL>システム: \"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5b77461-dd73-4926-8c9e-90d373f3046f",
      "metadata": {
        "id": "d5b77461-dd73-4926-8c9e-90d373f3046f"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 訓練済ファイルをダウンロード\n",
        "# トークナイザのダウンロードして tokenizer に格納\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt-neox-3.6b-instruction-sft-v2\", use_fast=False)\n",
        "\n",
        "# モデル本体のダウンロードとし model に格納\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt-neox-3.6b-instruction-sft-v2\")\n",
        "\n",
        "# GPU の使用\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(\"cuda\")\n",
        "\n",
        "# 対話文をトークン ID に変換して token_ids に格納\n",
        "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    # モデルにトークン ID を与えて output_ids を得る\n",
        "    output_ids = model.generate(\n",
        "        token_ids.to(model.device),\n",
        "        do_sample=True,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.7,\n",
        "        repetition_penalty=1.1,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "#output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
        "#output = output.replace(\"<NL>\", \"\\n\")\n",
        "#print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52254ab6-b5df-4905-b7ef-e76aaa518d9f",
      "metadata": {
        "id": "52254ab6-b5df-4905-b7ef-e76aaa518d9f"
      },
      "outputs": [],
      "source": [
        "def get_chatGPT_output(prompt:list):\n",
        "    \"\"\"chatGPT に prompt を与えて，出力を得る\"\"\"\n",
        "\n",
        "    # プロンプト文を tokenizer に与えて，トークン ID を得る\n",
        "    token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "    # トークン ID をモデルに与えて output_ids を得る\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            token_ids.to(model.device),\n",
        "            do_sample=True,\n",
        "            max_new_tokens=128,\n",
        "            temperature=0.7,\n",
        "            repetition_penalty=1.1,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            bos_token_id=tokenizer.bos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # output_ids を tokenizer に通してトークンを復号化\n",
        "    output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
        "\n",
        "    # 復号化した文の <NL> を 改行コードに置き換える\n",
        "    output = output.replace(\"<NL>\", \"\\n\")\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a4a3a28-3588-416d-a5bc-e2acb84b5189",
      "metadata": {
        "id": "5a4a3a28-3588-416d-a5bc-e2acb84b5189"
      },
      "outputs": [],
      "source": [
        "# Python では [] で囲まれたデータをリスト list と呼びます。\n",
        "# {} で囲まれたデータを辞書 dict と呼びます。\n",
        "# したがって，以下のデータは，5 つの辞書からなるリストを prompt という名前で定義しています。\n",
        "prompt0 = [\n",
        "    {\n",
        "        \"speaker\": \"ユーザー\",\n",
        "        \"text\": \"コンタクトレンズを慣れるにはどうすればよいですか？\"\n",
        "    },\n",
        "    {\n",
        "        \"speaker\": \"システム\",\n",
        "        \"text\": \"これについて具体的に説明していただけますか？何が難しいのでしょうか？\"\n",
        "    },\n",
        "    {\n",
        "        \"speaker\": \"ユーザー\",\n",
        "        \"text\": \"目が痛いのです。\"\n",
        "    },\n",
        "    {\n",
        "        \"speaker\": \"システム\",\n",
        "        \"text\": \"分かりました、コンタクトレンズをつけると目がかゆくなるということですね。思った以上にレンズを外す必要があるでしょうか？\"\n",
        "    },\n",
        "    {\n",
        "        \"speaker\": \"ユーザー\",\n",
        "        \"text\": \"いえ、レンズは外しませんが、目が赤くなるんです。\"\n",
        "    }\n",
        "]\n",
        "print(prompt0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "834c8958-2669-48d1-be59-039384d3dbbc",
      "metadata": {
        "id": "834c8958-2669-48d1-be59-039384d3dbbc"
      },
      "outputs": [],
      "source": [
        "# 上記のリストに対して，`speaker` と `text` というタグを使って，リスト prompt を `:` を使った形式に書き換える\n",
        "prompt1 = [\n",
        "    f\"{uttr['speaker']}: {uttr['text']}\"\n",
        "    for uttr in prompt0\n",
        "]\n",
        "print(prompt1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75d7cf2f-f882-4736-b583-cc9d18f36b70",
      "metadata": {
        "id": "75d7cf2f-f882-4736-b583-cc9d18f36b70"
      },
      "outputs": [],
      "source": [
        "# prompt を `<NL>` で繋ぐ\n",
        "prompt2 = \"<NL>\".join(prompt1)\n",
        "print(prompt2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "171647fc-2792-4d94-a981-3ac990dd09b9",
      "metadata": {
        "id": "171647fc-2792-4d94-a981-3ac990dd09b9"
      },
      "outputs": [],
      "source": [
        "# 最後に `システム:` をつけて，システムの出力を促す。\n",
        "prompt3 = (\n",
        "    prompt2\n",
        "    + \"<NL>\"\n",
        "    + \"システム: \"\n",
        ")\n",
        "print(prompt3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e5f7935-0a44-470c-8c7a-6f2eb0bf9192",
      "metadata": {
        "id": "9e5f7935-0a44-470c-8c7a-6f2eb0bf9192"
      },
      "outputs": [],
      "source": [
        "#token_ids = tokenizer.encode(prompt3, add_special_tokens=False, return_tensors=\"pt\")\n",
        "#print(token_ids.size())\n",
        "#print(tokenizer.convert_ids_to_tokens(token_ids.detach().numpy()[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1877b424-3a37-406e-8add-0326ea5a5cea",
      "metadata": {
        "id": "1877b424-3a37-406e-8add-0326ea5a5cea"
      },
      "outputs": [],
      "source": [
        "output = get_chatGPT_output(prompt3)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "id": "TqsA2OIzBxCf"
      },
      "id": "TqsA2OIzBxCf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ubi_KSUUBzIP"
      },
      "id": "Ubi_KSUUBzIP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}