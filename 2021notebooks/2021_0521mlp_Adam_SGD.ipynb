{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "2021_0521mlp_Adam_SGD.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/2015corona/blob/master/2021notebooks/2021_0521mlp_Adam_SGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8LPXv9c1pLY"
      },
      "source": [
        "# 3 層パーセプトロンと確率的勾配降下法のデモ\n",
        "\n",
        "- author: 浅川伸一\n",
        "- date: 2021_0521\n",
        "- filename: 2021_0521mlp_Adam_SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey80nbSS1pLd"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "np.set_printoptions(suppress=False, formatter={'float': '{:6.3f}'.format})\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImoprtError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib\n",
        "clear_output()    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6t1IWiY1pLd"
      },
      "source": [
        "def sigmoid(x, back=False):\n",
        "    if back:\n",
        "        return (x * (1. - x))\n",
        "    else:\n",
        "        return 1/(1+np.exp(-x))\n",
        "    \n",
        "def tanh(x, back=False):\n",
        "    if back:\n",
        "        return 1 - x ** 2\n",
        "    else:\n",
        "        return np.tanh(x)\n",
        "    \n",
        "def ReLU(x, back=False):\n",
        "    if back:\n",
        "        return ((x > 0) * 1.)\n",
        "    else:\n",
        "        return x * (x > 0)\n",
        "    \n",
        "\n",
        "class layer:\n",
        "    \n",
        "    def __init__(self, n_inp=2, n_out=4, act_f=tanh, lr=1e-1):\n",
        "        self.n_inp, self.out = n_inp, n_out\n",
        "        self.act_f = act_f\n",
        "        self.W = np.random.randn(n_out * n_inp).reshape(n_inp, n_out) / np.sqrt(n_inp + n_out)\n",
        "        self.bias = np.zeros((n_out,))\n",
        "        self.lr = lr\n",
        "        \n",
        "    def forward(self, X):\n",
        "        affine = X @ self.W + self.bias\n",
        "        return self.act_f(affine)\n",
        "\n",
        "    \n",
        "    def backward(self, dY, Y, X):\n",
        "        gradY = dY * self.act_f(Y, back=True)\n",
        "        dX = gradY @ self.W.T\n",
        "        dW =  X.T @ gradY\n",
        "        #d_bias = dW.mean()\n",
        "        return dX, dW #, d_bias, gradY\n",
        "    \n",
        "    def update(self, dW):\n",
        "        self.W -= self.lr * dW\n",
        "        self.bias -= self.lr * dW.mean()\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGVkh7tp1pLe"
      },
      "source": [
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "Tch = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "n_hid = 6\n",
        "lr = 1e-1\n",
        "\n",
        "#LayerH = layer(n_out=n_hid, n_inp=2, act_f=sigmoid, lr=lr)\n",
        "#LayerH = layer(n_out=n_hid, n_inp=2, act_f=ReLU, lr=lr)\n",
        "LayerH = layer(n_out=n_hid, n_inp=2, act_f=tanh, lr=lr)\n",
        "LayerO = layer(n_out=1, n_inp=n_hid, act_f=sigmoid, lr=lr)\n",
        "\n",
        "epochs = 10 ** 3\n",
        "interval = epochs >> 2\n",
        "losses = []\n",
        "for epoch in range(epochs+1):\n",
        "    H = LayerH.forward(X)\n",
        "    O = LayerO.forward(H)\n",
        "\n",
        "    deltaO = O - Tch\n",
        "    deltaH, dWo = LayerO.backward(deltaO, O, H)\n",
        "    deltaX, dWh = LayerH.backward(deltaH, H, X)\n",
        "    LayerO.update(dWo)\n",
        "    LayerH.update(dWh)\n",
        "\n",
        "    losses.append((deltaO ** 2).mean())\n",
        "    if epoch % interval == 0:\n",
        "        print(f'エポック:{epoch:>5d}, 損失値: {losses[-1]:.3f}', end=\" \")\n",
        "        print(f'出力:{O.T}')\n",
        "\n",
        "plt.ylim((0,0.28))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('エポック数')\n",
        "plt.ylabel('平均損失')\n",
        "plt.title(f'XOR デモ 中間層ニューロン数:{n_hid}, 学習係数:{lr}')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAUMjo2N1pLf"
      },
      "source": [
        "class XOR_demo():\n",
        "    def __init__(self, n_hid=8, lr=1e-1, act_f=tanh, max_epochs= 3 * 10 ** 3, interval=(10 ** 3)>>3):\n",
        "        \n",
        "        if act_f != None:\n",
        "            self.act_f = act_f\n",
        "            \n",
        "        self.X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "        self.Tch = np.array([[0],[1],[1],[0]])\n",
        "        \n",
        "        self.max_epochs = max_epochs\n",
        "        self.interval = interval\n",
        "        self.n_hid = n_hid\n",
        "        self.lr = lr\n",
        "\n",
        "        self.LayerH = layer(n_out=n_hid, n_inp=2, act_f=self.act_f, lr=lr)\n",
        "        self.LayerO = layer(n_out=1, n_inp=n_hid, act_f=sigmoid, lr=lr)\n",
        "\n",
        "\n",
        "    def fit(self, max_epochs=None, interval=None, draw_graph=True, title=\"\", verbose=False):\n",
        "        if max_epochs != None:\n",
        "            self.max_epochs = max_epochs\n",
        "        if interval != None:\n",
        "            self.interval = interval\n",
        "            \n",
        "        losses = []\n",
        "        for epoch in range(self.max_epochs+1):\n",
        "            #順伝播\n",
        "            H = self.LayerH.forward(X)\n",
        "            O = self.LayerO.forward(H)\n",
        "\n",
        "            #逆伝播\n",
        "            deltaO = (O - Tch) \n",
        "            deltaH, dWo = self.LayerO.backward(deltaO, O, H)\n",
        "            deltaX, dWh = self.LayerH.backward(deltaH, H, X)\n",
        "            \n",
        "            #パラメータの更新\n",
        "            self.LayerO.update(dWo)\n",
        "            self.LayerH.update(dWh)\n",
        "\n",
        "            losses.append((deltaO ** 2).mean())\n",
        "            if epoch % self.interval == 0 and verbose:\n",
        "                print(f'エポック:{epoch:>5d}, 損失: {losses[-1]:.3f}', end=\" \")\n",
        "                print(f'出力:{O.T}')\n",
        "\n",
        "        if draw_graph:\n",
        "            plt.ylim((0,0.28))\n",
        "            plt.plot(losses)\n",
        "            plt.xlabel('エポック数')\n",
        "            plt.ylabel('平均損失')\n",
        "            plt.title(title) # f'XOR デモ 中間層ニューロン数:{n_hid}, 学習係数:{lr}')\n",
        "            plt.show()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhcYtzyu1pLf"
      },
      "source": [
        "demo = XOR_demo(act_f=sigmoid)\n",
        "demo.fit(title=\"活性化関数:シグモイド\")\n",
        "\n",
        "demo2 = XOR_demo(act_f=tanh)\n",
        "demo2.fit(title=\"活性化関数:tanh\")\n",
        "\n",
        "demo3 = XOR_demo(act_f=ReLU, )\n",
        "demo3.fit(title=\"活性化関数:ReLU\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocWn9wP51pLg"
      },
      "source": [
        "# SGD と ADAM の比較\n",
        "\n",
        "<!-- - source: https://github.com/jrios6/Adam-vs-SGD-Numpy.git-->\n",
        "<!--This is a response to Siraj Raval's [Coding Challenge](https://github.com/llSourcell/The_evolution_of_gradient_descent/) to implement the Adam Optimization Strategy. \n",
        "ta), and comparing the performance difference between a standard Stochastic Gradient Descent and Adam.\n",
        "-->\n",
        "\n",
        "参照 URL\n",
        "1. [Adam: A method for Stochastic Optimization](https://arxiv.org/abs/1412.6980) by Diederik P. Kingma, Jimmy Ba  \n",
        "2. [CS231: Neural Networks](http://cs231n.github.io/neural-networks-3/#update) by Andrej Karpathy\n",
        "3. [Optimizing Gradient Descent](http://sebastianruder.com/optimizing-gradient-descent/index.html#adam) by Sebastian Ruder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4plQ6nX1pLg"
      },
      "source": [
        "## 以前と同様のデータセットを準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi8CORz-1pLg"
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import requests\n",
        "\n",
        "mnist_urls = {\n",
        "    #http://yann.lecun.com/exdb/mnist/\n",
        "    'Xtrain': 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
        "    'Ytrain': 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
        "    'Xtest': 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
        "    'Ytest':'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
        "}\n",
        "\n",
        "fashionmnist_urls = {\n",
        "    #https://github.com/zalandoresearch/fashion-mnist\n",
        "    'Xtest': 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz',\n",
        "    'Ytest': 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz',\n",
        "    'Xtrain': 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz',\n",
        "    'Ytrain': 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz'\n",
        "}\n",
        "\n",
        "kmnist_urls = {\n",
        "    #http://codh.rois.ac.jp/kmnist/\n",
        "    'Xtrain': 'http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz',\n",
        "    'Ytrain': 'http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz',\n",
        "    'Xtest': 'http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz',\n",
        "    'Ytest': 'http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz'\n",
        "}\n",
        " \n",
        "\n",
        "def download_mnist(dataset):\n",
        "    #上で定義したデータセットの情報を元にデータをダウンロードする\n",
        "    for name, url in dataset.items():\n",
        "        fname = url.split('/')[-1]\n",
        "        print(url, fname)\n",
        "        r = requests.get(url, timeout=35) #timeout=None はサーバからの応答が遅い場合永遠に待ち続ける\n",
        "        with open(fname, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "\n",
        "#データを変更して，繰り返し実行する際には，次行行頭の # を削除して，上のセルを再実行する必要があります\n",
        "# !rm *.gz\n",
        "# !ls -l *.gz*\n",
        "\n",
        "mnist_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "fashionmnist_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat' , \\\n",
        "                       'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "kmnist_labels = ['お', 'き', 'す', 'つ', 'な', 'は', 'ま', 'や', 'れ', 'を']\n",
        "# '0,U+304A,お', '1,U+304D,き', '2,U+3059,す', '3,U+3064,つ', '4,U+306A,な', \n",
        "# '5,U+306F,は', '6,U+307E,ま', '7,U+3084,や', '8,U+308C,れ', '9,U+3092,を'\n",
        "\n",
        "labels = mnist_labels\n",
        "labels = fashionmnist_labels\n",
        "labels = kmnist_labels\n",
        "\n",
        "#以下の 3 つのデータセットのうち 1 つを選んで実習してみましょう\n",
        "dataset = mnist_urls\n",
        "#dataset = fashionmnist_urls\n",
        "dataset = kmnist_urls\n",
        "\n",
        "labels = mnist_labels\n",
        "#labels = fashionmnist_labels\n",
        "labels = kmnist_labels\n",
        "\n",
        "download_mnist(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vgYWMUK1pLh"
      },
      "source": [
        "def load_mnist(path, kind='train'):\n",
        "    \"\"\"ダウンロードしたデータを読み込む関数\"\"\"\n",
        "    import os\n",
        "    import gzip\n",
        "    import numpy as np\n",
        "\n",
        "    \"\"\"Load MNIST data from `path`\"\"\"\n",
        "    labels_path = os.path.join(path,\n",
        "                               '%s-labels-idx1-ubyte.gz'\n",
        "                               % kind)\n",
        "    images_path = os.path.join(path,\n",
        "                               '%s-images-idx3-ubyte.gz'\n",
        "                               % kind)\n",
        "\n",
        "    with gzip.open(labels_path, 'rb') as lbpath:\n",
        "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "                               offset=8)\n",
        "\n",
        "    with gzip.open(images_path, 'rb') as imgpath:\n",
        "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "                               offset=16).reshape(len(labels), 784)\n",
        "\n",
        "    return images, labels"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqdybBbT1pLh"
      },
      "source": [
        "#データの表示\n",
        "import matplotlib.pyplot as plt\n",
        "X_train, Y_train = load_mnist('.', kind='train')\n",
        "X_test, Y_test = load_mnist('.', kind='t10k')\n",
        "\n",
        "_Y = np.zeros((len(Y_train),10))\n",
        "for i in range(len(_Y)):\n",
        "    _Y[i,Y_train[i]] = 1\n",
        "Y_train = _Y\n",
        "\n",
        "_Y = np.zeros((len(Y_test),10))\n",
        "for i in range(len(_Y)):\n",
        "    _Y[i,Y_test[i]] = 1\n",
        "Y_test = _Y\n",
        "\n",
        "# 時間節約のためデータ数を制限\n",
        "n_train = 3000  # 訓練データ数\n",
        "n_val = 500     # 検証データ数\n",
        "n_test = 500    # テストデータ数\n",
        "X_train = X_train[-n_train:]\n",
        "Y_train = Y_train[-n_train:]\n",
        "X_val = X_train[-n_val:]\n",
        "Y_val = Y_train[-n_val:]\n",
        "X_test = X_test[-n_test:]\n",
        "Y_test = Y_test[-n_test:]\n",
        "\n",
        "#次行の数字を変更して実施してください。ただし数字の範囲は 0 から 59999 までです\n",
        "No = int(input('次行の数字を変更して実施してください。ただし数字の範囲は 0 から 59999 までです:'))\n",
        "#No = 666\n",
        "plt.figure(figsize=(2,2))    #表示する縦横の大きさ，単位はインチ\n",
        "plt.title('label:{}'.format(labels[np.argmax(Y_train[No])]))\n",
        "plt.axis(False)\n",
        "plt.imshow(X_train[No].reshape(28,28), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi1rNv421pLh"
      },
      "source": [
        "## 3 層ニューラルネットワーク MLP の定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnVqJXl81pLi"
      },
      "source": [
        "class MLP3:\n",
        "    def __init__(self, n_inp, n_hid, n_out, lr, act_f=tanh):\n",
        "        \n",
        "        if act_f == None:\n",
        "            act_f = tanh\n",
        "\n",
        "        self.n_inp = n_inp  # 入力層 ニューロン(特徴）数\n",
        "        self.n_hid = n_hid  # 中間層 ニューロン数\n",
        "        self.n_out = n_out  # 出力層 ニューロン数\n",
        "        self.act_f = act_f  # 活性化関数\n",
        "        self.lr = lr        # 学習率\n",
        "        self.V_m, self.U_m = 0, 0  # 1 次のモーメント, 慣性モーメント\n",
        "        self.V_v, self.U_v = 0, 0  # 2 次のモーメント, 速度ベロシティのモーメンタム \n",
        "        self.t = 0\n",
        "        \n",
        "        # 結合係数行列の初期化\n",
        "        self.U = np.random.randn((n_inp * n_hid)).reshape(n_inp, n_hid) / np.sqrt(n_inp + n_hid)\n",
        "        self.V = np.random.randn((n_hid * n_out)).reshape(n_hid, n_out) / np.sqrt(n_hid + n_out)\n",
        "\n",
        "\n",
        "    def train(self, X, Tch, optimizer, decay1 = None, decay2 = None, epsilon = 1e-7):\n",
        "        # 順伝播\n",
        "        H = self.act_f(X @ self.U)\n",
        "        Y = self.act_f(H @ self.V)\n",
        "        \n",
        "        # 誤差逆伝播\n",
        "        Y_delta = Y - Tch\n",
        "        Y_grad  = Y_delta * self.act_f(Y, back=True)\n",
        "        H_delta = Y_grad @ self.V.T\n",
        "        H_grad  = H_delta * self.act_f(H, back=True)\n",
        "        \n",
        "        if optimizer == 'sgd':\n",
        "            # Update Weights\n",
        "            self.V -= self.lr * H.T @ Y_grad\n",
        "            self.U -= self.lr * X.T @ H_grad\n",
        "            \n",
        "        if optimizer == 'adam':\n",
        "            # Gradients for each layer\n",
        "            g1 = H.T.dot(Y_grad)\n",
        "            g0 = X.T.dot(H_grad)\n",
        "            \n",
        "            self.t += 1         # 時刻の更新\n",
        "            \n",
        "            # 中間層と出力層とに対する慣性モーメントを計算\n",
        "            self.V_m = self.V_m * decay1 + (1 - decay1) * g1\n",
        "            self.U_m = self.U_m * decay1 + (1 - decay1) * g0\n",
        "            \n",
        "            # 中間層と出力層とに対する 2 次モーメントを計算\n",
        "            self.V_v = self.V_v * decay2 + (1 - decay2) * (g1 ** 2)\n",
        "            self.U_v = self.U_v * decay2 + (1 - decay2) * (g0 ** 2)\n",
        "            \n",
        "            # 出力層モーメントへの時刻補正\n",
        "            V_mc = self.V_m / (1 - (decay1 ** self.t))\n",
        "            V_vc = self.V_v / (1 - (decay2 ** self.t))\n",
        "            \n",
        "            # 入力層モーメントへの時刻補正\n",
        "            U_mc = self.U_m / (1 - (decay1 ** self.t))\n",
        "            U_vc = self.U_v / (1 - (decay2 ** self.t))\n",
        "            \n",
        "            # パラメータ更新\n",
        "            dV = V_mc / (np.sqrt(V_vc) + epsilon)\n",
        "            dU = U_mc / (np.sqrt(U_vc) + epsilon)\n",
        "            \n",
        "            self.V -= self.lr * dV\n",
        "            self.U -= self.lr * dU\n",
        "            \n",
        "    def run(self, X):\n",
        "        H = self.act_f(X @ self.U)\n",
        "        Y = self.act_f(H @ self.V)\n",
        "        \n",
        "        return Y\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enT64Gwc1pLi"
      },
      "source": [
        "def MSE(y, Y):\n",
        "    return np.mean((y-Y)**2)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtNG_VLP1pLi"
      },
      "source": [
        "import time\n",
        "import sys\n",
        "\n",
        "def build_and_run(network, epochs=100, optimizer='adam', n_batch=64):\n",
        "    losses = {'train':[], 'validation':[]} # For Plotting of MSE\n",
        "    start = time.time()\n",
        "    interval = epochs >> 3\n",
        "    \n",
        "    # Iterating Over Epochs\n",
        "    for i in range(epochs):\n",
        "        \n",
        "        if optimizer == 'sgd':\n",
        "            # Iterating over mini batches\n",
        "            for k in range(X_train.shape[0] // n_batch):\n",
        "                batch = np.random.choice(len(X_train), size=n_batch)\n",
        "                X, y = X_train[batch], Y_train[batch]\n",
        "                network.train(X, y, optimizer)\n",
        "                train_loss = MSE(network.run(X_train), Y_train)\n",
        "                val_loss = MSE(network.run(X_val), Y_val)\n",
        "            if i % interval == 0:\n",
        "                print(f'エポック数:{i:05d}, 訓練損失:{train_loss:0.3f}, 検証損失: {val_loss:0.3f}')\n",
        "                \n",
        "        if optimizer == 'adam':\n",
        "            # Iterating over mini batches\n",
        "            for k in range(X_train.shape[0]// n_batch):\n",
        "                batch = np.random.choice(len(X_train), size=n_batch)\n",
        "                X, y = X_train[batch], Y_train[batch]\n",
        "                #print(f'30 X.shape: {X.shape}, y.shape: {y.shape}')\n",
        "                network.train(X, y, optimizer,\n",
        "                              decay1 = 0.9, decay2 = 0.99,\n",
        "                              epsilon = 10e-8)\n",
        "                train_loss = MSE(network.run(X_train), Y_train)\n",
        "                val_loss = MSE(network.run(X_val), Y_val)\n",
        "            if i % interval == 0:\n",
        "                print(f'エポック数:{i:05d}, 訓練損失:{train_loss:0.3f}, 検証損失: {val_loss:0.3f}')\n",
        "\n",
        "        losses['train'].append(train_loss)\n",
        "        losses['validation'].append(val_loss)\n",
        "        \n",
        "    print(f'総計算時間:{time.time()-start:.4f} 秒')\n",
        "    return losses"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfIrGsj41pLi",
        "outputId": "c2feb5fc-9846-4efe-8ca0-a6c1e05627a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "epochs = 101\n",
        "lr = 1e-4\n",
        "n_hid = 128\n",
        "\n",
        "n_out = 10\n",
        "batch_size = 128\n",
        "\n",
        "act_f = tanh\n",
        "print('活性化関数:tanh 最適化手法:Adam')\n",
        "model_tanh_adam = MLP3(X_train.shape[1], n_hid, n_out, lr, act_f=act_f)\n",
        "losses_adam_tanh = build_and_run(model_tanh_adam, epochs, 'adam', batch_size)\n",
        "\n",
        "print('活性化関数:tanh 最適化手法:SGD')\n",
        "model_tanh_sgd = MLP3(X_train.shape[1], n_hid, n_out, lr, act_f=act_f)\n",
        "losses_sgd_tanh = build_and_run(model_tanh_sgd, epochs, 'sgd', batch_size)\n",
        "\n",
        "act_f = sigmoid\n",
        "print('活性化関数:シグモイド 最適化手法:Adam')\n",
        "model_sigmoid_adam = MLP3(X_train.shape[1], n_hid, n_out, lr, act_f=act_f)\n",
        "losses_adam_sigmoid = build_and_run(model_sigmoid_adam, epochs, 'adam', batch_size)\n",
        "\n",
        "print('活性化関数:シグモイド 最適化手法:SGD')\n",
        "model_sigmoid_sgd = MLP3(X_train.shape[1], n_hid, n_out, lr, act_f=act_f)\n",
        "losses_sgd_sigmoid = build_and_run(model_sigmoid_sgd, epochs, 'sgd', batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "活性化関数:tanh 最適化手法:Adam\n",
            "エポック数:00000, 訓練損失:0.350, 検証損失: 0.350\n",
            "エポック数:00012, 訓練損失:0.180, 検証損失: 0.182\n",
            "エポック数:00024, 訓練損失:0.125, 検証損失: 0.125\n",
            "エポック数:00036, 訓練損失:0.094, 検証損失: 0.094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2_WIK8o1pLj"
      },
      "source": [
        "plt.plot(losses_adam_tanh['train'], label='Adam(tanh) 訓練データ損失')\n",
        "plt.plot(losses_adam_tanh['validation'], label='Adam(tahn) 検証データ損失')\n",
        "\n",
        "plt.plot(losses_sgd_tanh['train'], label='SGD(tanh) 訓練データ損失')\n",
        "plt.plot(losses_sgd_tanh['validation'], label='SGD(tanh) 検証データ損失')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh8J4ldd1pLj"
      },
      "source": [
        "<!--\n",
        "From the plots, we can observed that using Adam, weights of the neural network are more smoothly adjusted to reduce the training loss. \n",
        "Try increasing the learning rate, and you can see that Adam converges much faster compared to SGD, using an adaptive learning rate.\n",
        "\n",
        "The benefits of using Adam are not so obvious as the size of the data is very small and increasing training epochs tend to lead to overfitting and early-stopping is required. \n",
        "It is recommended to set the epochs for Adam to around 200 for the above hyperparameters configuration, as the training and validation loss starts diverging. \n",
        "However, we kept the epochs for both networks the same for plotting.\n",
        "\n",
        "Lastly, in this implementation, Adam is much faster to compute compared to SGD as it is processed as an entire training batch.  \n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyqH6FDz1pLj"
      },
      "source": [
        "## テストデータ による検証"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0xuEtkr1pLj"
      },
      "source": [
        "def test_model(network):\n",
        "    test_predictions = network.run(X_test)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i in range(len(test_predictions)):\n",
        "        total += 1\n",
        "        if np.argmax(test_predictions[i]) ==  np.argmax(Y_test[i]):\n",
        "            correct += 1\n",
        "    return correct/total\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qGjMkgy1pLj"
      },
      "source": [
        "print('テストデータセットに対する精度')\n",
        "print(f'Adam (シグモイド) : {test_model(model_sigmoid_adam)}')\n",
        "print(f'SGD  (シグモイド) : {test_model(model_sigmoid_sgd)}')\n",
        "print(f'Adam (tahn)     : {test_model(model_sigmoid_adam)}')\n",
        "print(f'SGD  (tanh)     : {test_model(model_sigmoid_sgd)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DaQGq-eGokf"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}