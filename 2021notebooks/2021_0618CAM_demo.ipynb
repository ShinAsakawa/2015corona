{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/2015corona/blob/master/2021notebooks/2021_0618CAM_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTmjaG_B70gj"
      },
      "source": [
        "# CAM の実習\n",
        "\n",
        "<div align='right'>\n",
        "<a href='mailto:educ0233@komazawa-u.ac.jp'>Shin Aasakawa</a>, all rights reserved.<br>\n",
        "Date: 24/Sep/2021<br/>\n",
        "Appache 2.0 license<br/>\n",
        "</div>\n",
        "\n",
        "* 元論文: https://arxiv.org/abs/1512.04150\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQfuHUiSGy5p"
      },
      "outputs": [],
      "source": [
        "# source: pytorchCAM.py\n",
        "# simple implementation of CAM in PyTorch for the networks such as ResNet, DenseNet, SqueezeNet, Inception\n",
        "\n",
        "import io\n",
        "import requests\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pdb\n",
        "\n",
        "\n",
        "# 使用する CNN モデルの名前を指定します。\n",
        "# 以下の 3 つの中から選んでください。\n",
        "# CAM はアベレージプーリング層を持つモデルに適用されます。\n",
        "#model_name = 'squeezenet'\n",
        "model_name = 'resnet'\n",
        "#model_name = 'densenet'\n",
        "if model_name == 'squeezenet':\n",
        "    net = models.squeezenet1_1(pretrained=True)\n",
        "    finalconv_name = 'features' # this is the last conv layer of the network\n",
        "elif model_name == 'resnet':\n",
        "    net = models.resnet18(pretrained=True)\n",
        "    finalconv_name = 'layer4'\n",
        "elif model_name == 'densenet':\n",
        "    net = models.densenet161(pretrained=True)\n",
        "    finalconv_name = 'features'\n",
        "\n",
        "# ネットワーク構造を表示します。最終畳み込み層の名前に注目してください\n",
        "net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qEZXC9nJGy5q"
      },
      "outputs": [],
      "source": [
        "# PyTorch で中間層の出力結果を得るためには，register_hook 関数を使う必要があります\n",
        "# 以下はそのための変数 `featre_blobs` を定義し，register_forward_hook 関数でその値を取り出す準備をしています。\n",
        "features_blobs = []\n",
        "def hook_feature(module, input, output):\n",
        "    features_blobs.append(output.data.cpu().numpy())\n",
        "\n",
        "net._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
        "\n",
        "# get the softmax weight\n",
        "params = list(net.parameters())\n",
        "# params[-1] が最終ソフトマックス層，params[-2] がソフトマックス層への重み係数\n",
        "weight_softmax = np.squeeze(params[-2].data.numpy())\n",
        "\n",
        "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
        "    \"\"\"CAM の実行\n",
        "    最終直下層 feature_conv と最終層のソフトマックスとの係数とから \n",
        "    CAM (クラス活性化マッピング) の値を計算して返す\n",
        "    \"\"\"\n",
        "    size_upsample = (256, 256)  # CAM の アンサンプリグ地図 (256x256) を作成\n",
        "    bz, nc, h, w = feature_conv.shape  # バッチサイズ，チャンネル数，高さ，幅の 4 次元テンソル\n",
        "    output_cam = []\n",
        "    for idx in class_idx:\n",
        "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
        "        cam = cam.reshape(h, w)\n",
        "        cam = cam - np.min(cam)\n",
        "        cam_img = cam / np.max(cam)\n",
        "        cam_img = np.uint8(255 * cam_img)\n",
        "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
        "    return output_cam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDM-6pGNGy5q"
      },
      "outputs": [],
      "source": [
        "# サンプル画像を読み込んで表示\n",
        "import IPython\n",
        "from PIL import Image as pil_img\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "import japanize_matplotlib\n",
        "%matplotlib inline\n",
        "\n",
        "# ImageNet の正解ラベル読み込み\n",
        "LABELS_ja_URL= 'https://gist.githubusercontent.com/PonDad/4dcb4b242b9358e524b4ddecbee385e9/raw/dda9454f74aa4fafee991ca8b848c9ab6ae0e732/imagenet_class_index.json'\n",
        "classes_ja = {int(i):val['ja'] for i, val in enumerate(requests.get(LABELS_ja_URL).json())}\n",
        "# LABELS_URL = 'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'\n",
        "\n",
        "\n",
        "# 入力画像\n",
        "IMG_URL = 'http://media.mlive.com/news_impact/photo/9933031-large.jpg'\n",
        "\n",
        "response = requests.get(IMG_URL)\n",
        "img_pil = Image.open(io.BytesIO(response.content))\n",
        "img_pil.save('test.jpg')\n",
        "\n",
        "img = plt.imread('test.jpg')\n",
        "plt.figure(figsize=(14,14))\n",
        "plt.title('元画像', fontsize=24)\n",
        "plt.axis('off'); plt.imshow(img)\n",
        "# IPython.display.Image(url=IMG_URL) # IPython による画像の画面表示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kltmcly4Gy5r"
      },
      "outputs": [],
      "source": [
        "# ImageNet のための画像認識モデルへの入力のため，画像のサイズと正規化の定義\n",
        "normalize = transforms.Normalize(\n",
        "   mean=[0.485, 0.456, 0.406],\n",
        "   std=[0.229, 0.224, 0.225]\n",
        ")\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "   transforms.Resize((224,224)),\n",
        "   transforms.ToTensor(),\n",
        "   normalize\n",
        "])\n",
        "\n",
        "\n",
        "img_tensor = preprocess(img_pil) # 前処理\n",
        "img_variable = Variable(img_tensor.unsqueeze(0))  # PyTorch の variable にする\n",
        "logit = net(img_variable) # 認識実行。結果を logit に代入\n",
        "\n",
        "h_x = F.softmax(logit, dim=1).data.squeeze() # ソフトマックス\n",
        "probs, idx = h_x.sort(0, True) # 認識結果から，カテゴリの確率を得る\n",
        "probs = probs.numpy()  # PyTorch から numpy 配列へ変換\n",
        "idx = idx.numpy()\n",
        "\n",
        "# 予測結果の出力\n",
        "top_n = 10\n",
        "print('認識結果')\n",
        "for i in range(0, top_n):\n",
        "    print(f'順位{i:2d} {classes_ja[idx[i]]} 確率:{probs[i]:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw40-cu3Gy5s"
      },
      "outputs": [],
      "source": [
        "# トップ 1 に認識されたカテゴリの活性値を元にヒートマップを描画\n",
        "candidate = 0  # 順位 topn=0\n",
        "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[candidate]])\n",
        "\n",
        "# render the CAM and output\n",
        "print(f'CAM の出力 {candidate:2d}:{classes_ja[idx[candidate]]}')\n",
        "\n",
        "img = cv2.imread('test.jpg')  # 元画像の読み込み\n",
        "height, width, _ = img.shape  # 元画像の縦横サイズを取得\n",
        "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
        "result = heatmap * 0.3 + img * 0.5\n",
        "cv2.imwrite('CAM.jpg', result)\n",
        "#IPython.display.Image('CAM.jpg')\n",
        "img_ = plt.imread('CAM.jpg')\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.title(classes_ja[idx[candidate]], fontsize=24)\n",
        "plt.axis('off'); plt.imshow(img_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biK3Jsc6Gy5t"
      },
      "outputs": [],
      "source": [
        "\n",
        "height, width, _ = img.shape  # 元画像のサイズ\n",
        "\n",
        "# \b第 top_n 位までの認識結果を表示\n",
        "top_n = 5\n",
        "for i in range(top_n):\n",
        "    CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[i]])\n",
        "    print(f'CAM の出力 {i:2d}: {classes_ja[idx[i]]}')\n",
        "\n",
        "    heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
        "    img = cv2.imread('test.jpg')\n",
        "    result = heatmap * 0.3 + img * 0.5\n",
        "    out_fname = 'CAM'+str(i)+'.jpg'\n",
        "    #cv2.imwrite('CAM.jpg', result)\n",
        "    cv2.imwrite(out_fname,  result)\n",
        "    #IPython.display.Image('CAM.jpg')\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.title(classes_ja[idx[i]], fontsize=24); plt.axis(False)\n",
        "    plt.imshow(pil_img.open(out_fname))\n",
        "    #plt.imshow(pil_img.open('CAM.jpg'))\n",
        "    #plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_xcositxGy5u"
      },
      "outputs": [],
      "source": [
        "# まとめて関数化\n",
        "def CAM5(img_url, n=5):\n",
        "    response = requests.get(img_url)\n",
        "    img_pil = Image.open(io.BytesIO(response.content))\n",
        "    img_pil.save('test.jpg')\n",
        "    \n",
        "    img_tensor = preprocess(img_pil)\n",
        "    img_variable = Variable(img_tensor.unsqueeze(0))\n",
        "    logit = net(img_variable)\n",
        "\n",
        "    h_x = F.softmax(logit, dim=1).data.squeeze()\n",
        "    probs, idx = h_x.sort(0, True)\n",
        "    probs = probs.numpy()\n",
        "    idx = idx.numpy()\n",
        "\n",
        "    for cand in range(n):\n",
        "        # generate class activation mapping for the top1 prediction\n",
        "        CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[cand]])\n",
        "\n",
        "        # render the CAM and output\n",
        "        print(f'CAM の出力 {candidate}: {classes_ja[idx[cand]]} 確率:{probs[cand]:.3f}')\n",
        "\n",
        "        img = cv2.imread('test.jpg')\n",
        "        height, width, _ = img.shape\n",
        "        heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
        "        result = heatmap * 0.3 + img * 0.5\n",
        "        cv2.imwrite('CAM.jpg', result)\n",
        "        #IPython.display.Image('CAM.jpg')\n",
        "        plt.figure(figsize=(8,8))\n",
        "        plt.axis(False); plt.title(classes_ja[idx[cand]], fontsize=24)\n",
        "        plt.imshow(pil_img.open('CAM.jpg')); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z1HAKgtGy5v"
      },
      "outputs": [],
      "source": [
        "# テスト実行\n",
        "url_img = 'https://raw.githubusercontent.com/komazawa-deep-learning/komazawa-deep-learning.github.io/master/assets/2012AlexNetResult.jpg'\n",
        "CAM5(url_img)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vg8Qd3se5u1N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "2021_0618CAM_demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}