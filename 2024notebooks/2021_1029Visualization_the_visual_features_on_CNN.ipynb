{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/2015corona/blob/master/2024notebooks/2021_1029Visualization_the_visual_features_on_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e51ab6e-398a-43d9-860e-2e86977a10e5"
      },
      "source": [
        "# DNN の特徴検出器視覚化\n",
        "\n",
        "\n",
        "* data: 2021_1029\n",
        "* filename: `2021_1029Visualization_the_visual_features_on_CNN.ipynb`\n",
        "* author: 浅川伸一\n",
        "* lincense: MIT"
      ],
      "id": "2e51ab6e-398a-43d9-860e-2e86977a10e5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6c2107c-9d69-4a51-a1bd-bf703341aba0"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "except ImportError:\n",
        "    !pip install japanize_matplotlib\n",
        "    import japanize_matplotlib"
      ],
      "id": "e6c2107c-9d69-4a51-a1bd-bf703341aba0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8422dbd5-964c-4b3e-aac3-2cfd28d2c8b3"
      },
      "source": [
        "#!pip install japanize_matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ResNet18 という学習済みのモデルを読み込みます。\n",
        "from torchvision import models\n",
        "resnet = models.resnet18(weights='DEFAULT')\n",
        "\n",
        "# ResNet18 がどのような深層畳み込みニューラルネットワークで構成されているかを調べてみます\n",
        "resnet"
      ],
      "id": "8422dbd5-964c-4b3e-aac3-2cfd28d2c8b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ca0b2da-50fe-41cb-a6a6-2c779a4a3f55"
      },
      "source": [
        "上の結果の 第 1 行目は，次のように表示されます\n",
        "\n",
        "* (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "\n",
        "これは conv1 という名前の層で，\n",
        "\n",
        "* 入力特徴数: 3 すなわち，入力データはカラー画像なので，赤，緑，青の光の 3 原色のそれぞれを表した数値となっています\n",
        "* 出力特徴数: 64\n",
        "* カーネルサイズ: (7, 7) 7 画素 X 7 画素のカーネル幅を持つ畳み込み演算が行われることを示しています\n",
        "* ストライド: (2, 2) 縦と横とも 2 画素おきにずらしてカーネルが移動することを意味しています。\n",
        "* パディング: (3, 3) 上下左右に入力画像の周辺に 3 画素分だけ架空の入力領域を確保していることを表します"
      ],
      "id": "0ca0b2da-50fe-41cb-a6a6-2c779a4a3f55"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08584ac7-d89a-4213-ad86-0cae4297555a"
      },
      "source": [
        "# 次に ResNet18 の第一層，すなわち，最も入力に近い層のサイズを調べます。\n",
        "print(resnet.conv1.weight.size())\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.axis('off')\n",
        "plt.imshow(resnet.conv1.weight.detach().numpy()[0].transpose(1,2,0).clip(0,1), cmap='gray')"
      ],
      "id": "08584ac7-d89a-4213-ad86-0cae4297555a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20c0c195-9e4d-4dca-b80b-9e3f9826772a"
      },
      "source": [
        "# 上記の結果は，torch.Size([64, 3, 7, 7]) と表示されているはずです。\n",
        "# これは，4 次元のデータで，以下のような意味になります。\n",
        "# 64: 出力特徴数，\n",
        "# 3: 入力特徴数\n",
        "\n",
        "import torchvision\n",
        "\n",
        "def img_show(inp, title=None, figsize=(4,4), fontsize=12, **kwargs):\n",
        "    \"\"\"テンソルを画像として表示\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    #inp = inp.numpy().transpose((1, 2, 0))\n",
        "    #mean = np.array([0.485, 0.456, 0.406])\n",
        "    #std = np.array([0.229, 0.224, 0.225])\n",
        "    #inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(inp)\n",
        "    #plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title, fontsize=fontsize)\n",
        "\n",
        "# ResNet18\n",
        "data = resnet.conv1.weight.detach()\n",
        "out = torchvision.utils.make_grid(data)\n",
        "img_show(out, title=\"ResNet18 の最下位層の特徴検出器\")\n",
        "plt.show()"
      ],
      "id": "20c0c195-9e4d-4dca-b80b-9e3f9826772a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6ae41d9-23d0-4762-9bff-81da0b352e84"
      },
      "source": [
        "# AlexNet\n",
        "alexnet = models.alexnet(weights='DEFAULT')\n",
        "data = alexnet.features[0].weight.detach()\n",
        "out = torchvision.utils.make_grid(data)\n",
        "img_show(out, figsize=(4,4), title=\"AlexNet の最下位層の特徴検出器\")\n",
        "plt.show()"
      ],
      "id": "d6ae41d9-23d0-4762-9bff-81da0b352e84",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11f35cbc-293e-4696-878a-dcae8bee2067"
      },
      "source": [
        "# Inception_v3\n",
        "net = models.inception_v3(weights='DEFAULT')\n",
        "data = net.Conv2d_1a_3x3.conv.weight.detach()\n",
        "out = torchvision.utils.make_grid(data)\n",
        "img_show(out, figsize=(4,10), title=\"inception_v3 の最下位層の特徴検出器\")\n",
        "plt.show()"
      ],
      "id": "11f35cbc-293e-4696-878a-dcae8bee2067",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd43c9a3-3091-4615-9b2f-1837e32c73e1"
      },
      "source": [
        "# VGG11\n",
        "net = models.vgg11(weights='DEFAULT')\n",
        "data = net.features[0].weight.detach()\n",
        "out = torchvision.utils.make_grid(data)\n",
        "img_show(out, figsize=(4,4), title=\"VGG11 の最下位層の特徴検出器\")\n",
        "plt.show()"
      ],
      "id": "bd43c9a3-3091-4615-9b2f-1837e32c73e1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99e6affc-766f-4787-9bfd-07af2edbd44d"
      },
      "source": [
        "# densenet121\n",
        "net = models.densenet121(pretrained=True)\n",
        "data = net.features.conv0.weight.detach()\n",
        "out = torchvision.utils.make_grid(data)\n",
        "img_show(out, title=\"densnett121 の最下位層の特徴検出器\")\n",
        "plt.show()"
      ],
      "id": "99e6affc-766f-4787-9bfd-07af2edbd44d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "236a9ff8-83c8-4214-a807-8fcef610475e"
      },
      "source": [
        "# mobilenet_v2\n",
        "net = models.mobilenet_v2(pretrained=True)\n",
        "data = net.features[0][0].weight\n",
        "out = torchvision.utils.make_grid(data)\n",
        "img_show(out, figsize=(6,6), title=\"mobilenet の最下位層の特徴検出器\")\n",
        "plt.show()"
      ],
      "id": "236a9ff8-83c8-4214-a807-8fcef610475e",
      "execution_count": null,
      "outputs": []
    }
  ]
}