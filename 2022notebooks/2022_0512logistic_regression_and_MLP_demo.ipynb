{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022_0512logistic_regression_and_MLP_demo.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOpJze2D+okQP5EC2eECB5m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/2015corona/blob/master/2022notebooks/2022_0512logistic_regression_and_MLP_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ロジスティック回帰と多層パーセプトロンのデモ\n",
        "\n",
        "ここでは 2 次元のおもちゃのニューラルネットワークの完全な実装を見て行きます。\n",
        "まず単純な線形分類器を実装し，次にそのコードを 2 層のニューラルネットワークに拡張します。 \n",
        "これから見るように，この拡張は驚くほど簡単で，ほとんど変更する必要はありません。\n",
        "<!-- In this section we'll walk through a complete implementation of a toy Neural Network in 2 dimensions. \n",
        "We'll first implement a simple linear classifier and then extend the code to a 2-layer Neural Network. As we'll see, this extension is surprisingly simple and very few changes are necessary. -->\n",
        "\n",
        "<a name='data'></a>\n",
        "\n",
        "## 1. データ生成\n",
        "<!-- ## Generating some data -->\n",
        "\n",
        "線形分離が容易でない分類データセットを生成してみよう。\n",
        "我々の好きな例は、スパイラルデータセットで、次のように生成できる。\n",
        "<!-- Lets generate a classification dataset that is not easily linearly separable. \n",
        "Our favorite example is the spiral dataset, which can be generated as follows: -->\n"
      ],
      "metadata": {
        "id": "81FCvOvzn8d7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhHXHksnn7cv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython\n",
        "isColab = 'google.colab' in str(IPython.get_ipython())\n",
        "if isColab:\n",
        "    !pip install japanize_matplotlib\n",
        "import japanize_matplotlib\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (8.0, 8.0)     # グラフ描画領域の設定，単位はインチ\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "\n",
        "N = 100 # 一クラスあたりのデータ数\n",
        "D = 2   # 次元数\n",
        "K = 3   # クラス数\n",
        "X = np.zeros((N*K,D))            # データ行列 (一行に一データ)\n",
        "y = np.zeros(N*K, dtype='uint8') # クラスのラベル，すなわち教師信号\n",
        "for j in range(K):\n",
        "  ix = range(N*j,N*(j+1))\n",
        "  r = np.linspace(0.0,1,N) # radius\n",
        "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
        "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
        "  y[ix] = j\n",
        "# lets visualize the data:\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
        "plt.title('おもちゃの螺旋データは線形分離不可能な 3 つのクラス (青，赤，黄) で構成されている')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "通常は各特徴が平均 0 と単位標準偏差を持つようにデータセットを前処理したいところです。\n",
        "ですが，この場合，特徴はすでに -1〜1 のきれいな範囲にあるので，このステップは省略します。\n",
        "<!-- Normally we would want to preprocess the dataset so that each feature has zero mean and unit standard deviation, but in this case the features are already in a nice range from -1 to 1, so we skip this step. -->\n",
        "\n",
        "<a name='linear'></a>\n",
        "\n",
        "## 2. ソフトマックス分類器の訓練 \n",
        "<!-- Training a Softmax Linear Classifier -->\n",
        "\n",
        "<a name='init'></a>\n",
        "\n",
        "### 2.1 パラメータの初期化\n",
        "<!-- Initialize the parameters -->\n",
        "\n",
        "まず，このデータセットに対して ソフトマックス分類器を学習させましょう。\n",
        "ソフトマックス分類器はとは，線形結合による得点関数を持ち，交差エントロピーの損失関数を利用します。\n",
        "線形分類器のパラメータは，各クラスに対する重み行列 `W` とバイアスベクトル `b` から構成されます。\n",
        "まず，これらのパラメータを乱数として初期化しましょう。\n",
        "<!-- Lets first train a Softmax classifier on this classification dataset. \n",
        "As we saw in the previous sections, the Softmax classifier has a linear score function and uses the cross-entropy loss. \n",
        "The parameters of the linear classifier consist of a weight matrix `W` and a bias vector `b` for each class. Lets first initialize these parameters to be random numbers: -->\n"
      ],
      "metadata": {
        "id": "xP_JuqQjoTVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# パラメータを乱数で初期化\n",
        "W = 0.01 * np.random.randn(D,K)\n",
        "b = np.zeros((1,K))\n"
      ],
      "metadata": {
        "id": "bS2_ugjAoRUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここで `D = 2` が次元数，`K = 3` がクラス数であることを覚えておいてください。\n",
        "<!-- Recall that we `D = 2` is the dimensionality and `K = 3` is the number of classes.  -->\n",
        "\n",
        "<a name='scores'></a>\n",
        "\n",
        "### 2.1. クラス得点の計算\n",
        "<!-- ### Compute the class scores -->\n",
        "\n",
        "作成するモデルは線形分類器ですので，1 回の行列の乗算ですべてのクラス得点を非簡単に計算することができます。\n",
        "<!-- Since this is a linear classifier, we can compute all class scores very simply in parallel with a single matrix multiplication: -->\n"
      ],
      "metadata": {
        "id": "P8jbKsbFpdHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute class scores for a linear classifier\n",
        "scores = np.dot(X, W) + b"
      ],
      "metadata": {
        "id": "5Cy6v85apcKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この例では 2 次元の点が 300 個あります。\n",
        "この乗算後の配列 `scores` の大きさは  [300 x 3] となります。\n",
        "各行には 3 つのクラス (青，赤，黄) に対応するクラスの得点が格納されます。\n",
        "<!-- In this example we have 300 2-D points, so after this multiplication the array `scores` will have size [300 x 3], where each row gives the class scores corresponding to the 3 classes (blue, red, yellow). -->\n",
        "\n",
        "<a name='loss'></a>\n",
        "\n",
        "### 2.2 損失値の計算\n",
        "<!-- ### Compute the loss -->\n",
        "\n",
        "2 番目に重要な要素は損失関数です。\n",
        "これは計算されたクラス得点を使用して不幸を定量化する微分可能な目的関数です。\n",
        "直感的には正しいクラスが他のクラスより高い得点になっていることが望ましいのです。\n",
        "このときの損失値は小さくなり，そうでない場合は大きくなります。\n",
        "この直感を定量化する方法はたくさんありますが，この例ではソフトマックス分類器に関連する交差エントロピーの損失を使用することにします。 \n",
        "ある 1 つの事例に対するクラス得点の配列 (ここでは一行に 3 つの数値をもつ配列) を $f(x)$  とするとソフトマックス分類器はその例に対する損失値を次のように計算することになります:\n",
        "<!-- The second key ingredient we need is a loss function, which is a differentiable objective that quantifies our unhappiness with the computed class scores. \n",
        "Intuitively, we want the correct class to have a higher score than the other classes. \n",
        "When this is the case, the loss should be low and otherwise the loss should be high. \n",
        "There are many ways to quantify this intuition, but in this example lets use the cross-entropy loss that is associated with the Softmax classifier. Recall that if \\\\(f\\\\) is the array of class scores for a single example (e.g. array of 3 numbers here), then the Softmax classifier computes the loss for that example as: -->\n",
        "\n",
        "$$\n",
        "L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right)\n",
        "$$\n",
        "\n",
        "ソフトマックス分類器は $f$ の各要素を 3 つのクラスの (非正規化) 対数確率を保持していると解釈していることがわかります。\n",
        "これらを指数化して (非正規化) 確率を求め，正規化して確率を求めます。\n",
        "したがって log の中の式は正しいクラスの正規化された確率となります。\n",
        "この式の仕組みに注意してください。この量は常に 0 と 1 の間にあります。\n",
        "正しいクラスの確率が非常に小さい (0 に近い) 場合，損失値は (正の) 無限大の方向に進みます。\n",
        "逆に，正しいクラスの確率が 1 に近づくと $\\log(1)=0$ となり，損失値は 0 に近づきます。\n",
        "したがって，正しいクラス確率が高いとき $L_{i}$ の式は小さくなり，低い場合は $L_{i}$ の値は非常に大きくなります。\n",
        "<!-- We can see that the Softmax classifier interprets every element of $f$ as holding the (unnormalized) log probabilities of the three classes. We exponentiate these to get (unnormalized) probabilities, and then normalize them to get probabilites. \n",
        "Therefore, the expression inside the log is the normalized probability of the correct class. \n",
        "Note how this expression works: this quantity is always between 0 and 1. \n",
        "When the probability of the correct class is very small (near 0), the loss will go towards (postiive) infinity. Conversely, when the correct class probability goes towards 1, the loss will go towards zero because $log(1) = 0\\$. \n",
        "Hence, the expression for \\\\(L_i\\\\) is low when the correct class probability is high, and it's very high when it is low.  -->\n",
        "\n",
        "また，ソフトマックス分類器の損失値は，学習例と正則化に対する平均交差エントロピー損失として定義されることを覚えておいてください。\n",
        "<!-- Recall also that the full Softmax classifier loss is then defined as the average cross-entropy loss over the training examples and the regularization: -->\n",
        "\n",
        "$$\n",
        "L = \\underbrace{\\frac{1}{N}\\sum_i L_i }_\\text{データ損失} + \\underbrace{\\frac{1}{2} \\lambda \\sum_k\\sum_l W_{k,l}^2 }_\\text{正規化損失値} \\\\\n",
        "$$\n",
        "\n",
        "上で計算した `scores` の配列があれば，損失値を計算することができます。\n",
        "まず，確率の求め方は簡単です。\n",
        "<!-- Given the array of `scores` we've computed above, we can compute the loss. \n",
        "First, the way to obtain the probabilities is straight forward: -->\n"
      ],
      "metadata": {
        "id": "grP7UCMwqDet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 正規化されていない得点\n",
        "exp_scores = np.exp(scores)\n",
        "\n",
        "# 各事例に対する得点を正規化して確率に変換\n",
        "probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "cs54h88CqCvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "これで大きさ [300 x 3] の配列 `probs` ができあがりました。\n",
        "各行にはクラスの確率が格納されます。\n",
        "特に，正規化したので，各行の合計は 1 になります。\n",
        "これで，各例で正しいクラスに割り当てられた対数確率を問い合わせることができます。\n",
        "<!-- We now have an array `probs` of size [300 x 3], where each row now contains the class probabilities. In particular, since we've normalized them every row now sums to one. \n",
        "We can now query for the  log probabilities assigned to the correct classes in each example: -->\n"
      ],
      "metadata": {
        "id": "MCoSiKHYtBbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples = X.shape[0]\n",
        "corect_logprobs = -np.log(probs[range(num_examples),y])"
      ],
      "metadata": {
        "id": "_atc_XpFtxDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "配列 `correct_logprobs` は各例に対して正しいクラスに割り当てられた確率だけを集めた 1次元配列です。\n",
        "全損失は，これらの対数確率と正則化損失の平均です。\n",
        "<!-- The array `correct_logprobs` is a 1D array of just the probabilities assigned to the correct classes for each example. The full loss is then the average of these log probabilities and the regularization loss:-->\n"
      ],
      "metadata": {
        "id": "bzXTVADTuI3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ハイパーパラメータの設定\n",
        "step_size = 1e-0\n",
        "_lambda = 1e-3 # regularization strength\n",
        "\n",
        "# 損失値の計算: 平均交差エントロピーと正則化項との和\n",
        "data_loss = np.sum(corect_logprobs)/num_examples\n",
        "reg_loss = 0.5 * _lambda * np.sum(W*W)\n",
        "loss = data_loss + reg_loss"
      ],
      "metadata": {
        "id": "7jolXtGQuTzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "このコードでは 正則化強度の $\\lambda$ が `_lambda` として格納されています。\n",
        "`0.5`という正則化の倍率が便利なのは，すぐにわかるでしょう。\n",
        "これは `np.log(1.0/3)` であり，小さな初期乱数重みでは，全てのクラスに割り当てられる確率は約 3 分の 1 になるからです。\n",
        "ここで，損失値をできるだけ小さくしたいと考え `loss = 0`を絶対的な下限とします。\n",
        "しかし，損失が小さいほど，すべての例について正しいクラスに割り当てられる確率が高くなります。\n",
        "<!-- In this code, the regularization strength $\\lambda$ is stored inside the `_lambda`. \n",
        "The convenience factor of `0.5` multiplying the regularization will become clear in a second. \n",
        "Evaluating this in the beginning (with random parameters) might give us `loss = 1.1`, which is `np.log(1.0/3)`, since with small initial random weights all probabilities assigned to all classes are about one third. We now want to make the loss as low as possible, with `loss = 0` as the absolute lower bound. \n",
        "But the lower the loss is, the higher are the probabilities assigned to the correct classes for all examples. -->\n",
        "\n",
        "<a name='grad'></a>\n",
        "\n",
        "### 2.3 誤差逆伝播法による解析的勾配の計算\n",
        "<!-- ### Computing the Analytic Gradient with Backpropagation-->\n",
        "\n",
        "損失を評価する方法ができたので，次はそれを最小化する必要があります。\n",
        "そのために勾配降下法を使います。\n",
        "つまり，ランダムなパラメータ (上図参照) から始めて，パラメータに対する損失関数の勾配を評価し，損失を減らすためにパラメータをどのように変更すればよいかを知ることができるのです。\n",
        "ここで，中間変数 $p$ を導入します。\n",
        "これは (正規化された) 確率ベクトルです。\n",
        "ある事例の損失値は以下のようになります:\n",
        "<!--\n",
        "We have a way of evaluating the loss, and now we have to minimize it. \n",
        "We'll do so with gradient descent. \n",
        "That is, we start with random parameters (as shown above), and evaluate the gradient of the loss function with respect to the parameters, so that we know how we should change the parameters to decrease the loss. \n",
        "Lets introduce the intermediate variable $(p$, which is a vector of the (normalized) probabilities. \n",
        "The loss for one example is: -->\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p_k &= \\frac{e^{f_k}}{ \\sum_j e^{f_j} } \\\\\n",
        "L_{i} &= -\\log\\left(p_{y_i}\\right)\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "ここで，この事例が全目的関数に寄与する損失値 $L_i$ を減少させるため，$f$ 内部の計算された得点がどのように変化すべきかを理解したいとしましょう。\n",
        "すなわち，勾配 $\\partial L_{i} / \\partial f_{k}$ を導出したいのです。\n",
        "損失値 $L_{i}$ は $p$ から計算されますが，これは $f$ に依存します。\n",
        "合成関数の微分公式を使って勾配を導くのは読者にとって楽しい練習ですが，いろいろなことが相殺されて，最終的には極めて簡潔で解釈しやすいことがわかります。\n",
        "<!-- We now wish to understand how the computed scores inside $f$ should change to decrease the loss $L_i$ that this example contributes to the full objective. \n",
        "In other words, we want to derive the gradient $\\partial L_{i} / \\partial f_{k}$. \n",
        "The loss $L_i$ is computed from $p$, which in turn depends on $f$. \n",
        "It's a fun exercise to the reader to use the chain rule to derive the gradient, but it turns out to be extremely simple and interpretible in the end, after a lot of things cancel out: -->\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L_i }{ \\partial f_k } = p_k - \\mathbb{1}(y_i = k)\n",
        "$$\n",
        "\n",
        "この式が，いかに簡潔で美しいことにに注目です。\n",
        "我々が計算した確率が `p = [0.2, 0.3, 0.5]` で，正しいクラスは (確率 0.3 の) 真ん中のクラスであったと仮定します。\n",
        "この導出によれば，得点の勾配は `df = [0.2, -0.7, 0.5]` となります。\n",
        "得点ベクトル `f` の最初か最後の要素 (不正確なクラスの得点) を増やすと，損失が **増大** します (\n",
        "正の符号  +0.2 と +0.5 のため)。\n",
        "このことは，すなわち損失値の増大とは，悪いことを意味します。\n",
        "<!-- Notice how elegant and simple this expression is. \n",
        "Suppose the probabilities we computed were `p = [0.2, 0.3, 0.5]`, and that the correct class was the middle one (with probability 0.3). \n",
        "According to this derivation the gradient on the scores would be `df = [0.2, -0.7, 0.5]`. \n",
        "Recalling what the interpretation of the gradient, we see that this result is highly intuitive: increasing the first or last element of the score vector `f` (the scores of the incorrect classes) leads to an *increased* loss (due to the positive signs +0.2 and +0.5) - and increasing the loss is bad, as expected. -->\n",
        "\n",
        "正しいクラスの得点を上げると，損失値に対して **負の効果** を及ぼします。\n",
        "勾配が -0.7 ということは，正しいクラスの得点を上げると損失 $L_{i}$ が減るということであり，これは理にかなっています。\n",
        "<!-- However, increasing the score of the correct class has *negative* influence on the loss. \n",
        "ppThe gradient of -0.7 is telling us that increasing the correct class score would lead to a decrease of the loss $L_{i}$, which makes sense.-->\n",
        "\n",
        "これらのことは，以下のコードに集約されます。\n",
        "`probs` は各例に対する全クラスの確率を (行として) 保存していることを思い出してください。\n",
        "得点の勾配を求めるためには `dscores` を次のように進めます。\n",
        "<!-- All of this boils down to the following code. Recall that `probs` stores the probabilities of all classes (as rows) for each example. \n",
        "To get the gradient on the scores, which we call `dscores`, we proceed as follows: -->\n"
      ],
      "metadata": {
        "id": "_IXjLCMiuFxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dscores = probs\n",
        "dscores[range(num_examples),y] -= 1\n",
        "dscores /= num_examples"
      ],
      "metadata": {
        "id": "-LN-vhmLqBN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後に `scores = np.dot(X, W) + b` があったので `dscores` に格納されている `scores` の勾配を利用して `W` と `b` に誤差逆伝播することができます。\n",
        "<!-- Lastly, we had that `scores = np.dot(X, W) + b`, so armed with the gradient on `scores` (stored in `dscores`), we can now backpropagate into `W` and `b`: -->\n"
      ],
      "metadata": {
        "id": "uqqHjMHwyTlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dW = np.dot(X.T, dscores)\n",
        "db = np.sum(dscores, axis=0, keepdims=True)\n",
        "dW += _lambda * W    # 正則化項の勾配"
      ],
      "metadata": {
        "id": "-cbM0lxhy26P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここで，行列の乗算演算を誤差逆伝播し，さらに正則化の寄与を追加していることがわかります。\n",
        "正則化勾配は，損失寄与に定数 `0.5` を用いたので，非常に単純な形 `_lambda * W` であることに注意してください  (i.e. $\\displaystyle\\frac{d}{dw} \\left( \\frac{1}{2}\\lambda w^2\\right) =\\lambda w$). \n",
        "これは勾配式を簡単にするよくある便宜的なトリックです。\n",
        "<!-- Where we see that we have backpropped through the matrix multiply operation, and also added the contribution from the regularization. \n",
        "Note that the regularization gradient has the very simple form `reg*W` since we used the constant `0.5` for its loss contribution (i.e. $\\displaystyle\\frac{d}{dw} \\left( \\frac{1}{2}\\lambda w^2\\right) =\\lambda w$. \n",
        "This is a common convenience trick that simplifies the gradient expression. -->\n",
        "\n",
        "<a name='update'></a>\n",
        "\n",
        "### 2.4 パラメータ更新の実行\n",
        "<!-- ### 2.4 Performing a parameter update-->\n",
        "\n",
        "勾配を評価したことで，すべてのパラメータが損失関数にどのように影響するかがわかりました。\n",
        "ここで，損失を **減らす** ために，**負の勾配方向** にパラメータ更新を実行することにします。\n",
        "<!-- Now that we've evaluated the gradient we know how every parameter influences the loss function. \n",
        "We will now perform a parameter update in the *negative* gradient direction to *decrease* the loss: -->\n"
      ],
      "metadata": {
        "id": "EDKH4-c2y07r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# パラメータの更新\n",
        "W += -step_size * dW\n",
        "b += -step_size * db"
      ],
      "metadata": {
        "id": "5-qZdHUy0iYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<a name='together'></a>\n",
        "\n",
        "### ソフトマックス分類器の全部盛り\n",
        "<!-- Putting it all together: Training a Softmax Classifier -->\n",
        "\n",
        "これらをまとめると、勾配降下法を用いたソフトマックス分類器の学習コードの全容は次のようになります。\n",
        "<!-- Putting all of this together, here is the full code for training a Softmax classifier with Gradient descent: -->\n",
        "\n"
      ],
      "metadata": {
        "id": "gl4dHUwd0ftT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train a Linear Classifier\n",
        "\n",
        "# initialize parameters randomly\n",
        "W = 0.01 * np.random.randn(D,K)\n",
        "b = np.zeros((1,K))\n",
        "\n",
        "# some hyperparameters\n",
        "step_size = 1e-0\n",
        "_lambda = 1e-3  # regularization strength\n",
        "\n",
        "# gradient descent loop\n",
        "num_examples = X.shape[0]\n",
        "for i in range(200):\n",
        "  \n",
        "    # evaluate class scores, [N x K]\n",
        "    scores = np.dot(X, W) + b \n",
        "  \n",
        "    # compute the class probabilities\n",
        "    exp_scores = np.exp(scores)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
        "  \n",
        "    # compute the loss: average cross-entropy loss and regularization\n",
        "    corect_logprobs = -np.log(probs[range(num_examples),y])\n",
        "    data_loss = np.sum(corect_logprobs)/num_examples\n",
        "    reg_loss = 0.5 * _lambda * np.sum(W*W)\n",
        "    loss = data_loss + reg_loss\n",
        "    if i % 10 == 0:\n",
        "        print(f\"繰り返し回数 {i:3d}: 損失値:{loss:.3f}\")\n",
        "  \n",
        "    # compute the gradient on scores\n",
        "    dscores = probs\n",
        "    dscores[range(num_examples),y] -= 1\n",
        "    dscores /= num_examples\n",
        "  \n",
        "    # backpropate the gradient to the parameters (W,b)\n",
        "    dW = np.dot(X.T, dscores)\n",
        "    db = np.sum(dscores, axis=0, keepdims=True)\n",
        "  \n",
        "    dW += _lambda * W # regularization gradient\n",
        "  \n",
        "    # perform a parameter update\n",
        "    W += -step_size * dW\n",
        "    b += -step_size * db\n"
      ],
      "metadata": {
        "id": "Bk1Naf5t09u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "結果は次のようになるでしょう:\n",
        "<!-- Running this prints the output: -->\n",
        "\n",
        "```\n",
        "繰り返し回数   0: 損失値:1.099\n",
        "繰り返し回数  10: 損失値:0.911\n",
        "繰り返し回数  20: 損失値:0.841\n",
        "繰り返し回数  30: 損失値:0.810\n",
        "繰り返し回数  40: 損失値:0.794\n",
        "繰り返し回数  50: 損失値:0.784\n",
        "繰り返し回数  60: 損失値:0.779\n",
        "繰り返し回数  70: 損失値:0.776\n",
        "繰り返し回数  80: 損失値:0.773\n",
        "繰り返し回数  90: 損失値:0.772\n",
        "繰り返し回数 100: 損失値:0.771\n",
        "繰り返し回数 110: 損失値:0.770\n",
        "繰り返し回数 120: 損失値:0.770\n",
        "繰り返し回数 130: 損失値:0.769\n",
        "繰り返し回数 140: 損失値:0.769\n",
        "繰り返し回数 150: 損失値:0.769\n",
        "繰り返し回数 160: 損失値:0.769\n",
        "繰り返し回数 170: 損失値:0.769\n",
        "繰り返し回数 180: 損失値:0.769\n",
        "繰り返し回数 190: 損失値:0.769\n",
        "```\n",
        "\n",
        "約 200 回の反復の後，何かに収束したことがわかります。\n",
        "訓練データセットの精度を評価することができます。\n",
        "<!-- We see that we've converged to something after about 190 iterations. We can evaluate the training set accuracy: -->\n",
        "\n"
      ],
      "metadata": {
        "id": "AmCSHWXn04UF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate training set accuracy\n",
        "scores = np.dot(X, W) + b\n",
        "predicted_class = np.argmax(scores, axis=1)\n",
        "print(f'訓練精度: {np.mean(predicted_class == y):.2f}%')\n"
      ],
      "metadata": {
        "id": "_isjzwXX1-xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "これは **49%**と表示されます。\n",
        "あまり良い結果ではありませんが，データセットが線形分離できないように構成されていることを考えると，驚くことではありません。\n",
        "また，学習された判定境界をプロットすることもできます。\n",
        "<!-- This prints 49%. Not very good at all, but also not surprising given that the dataset is constructed so it is not linearly separable. We can also plot the learned decision boundaries: -->"
      ],
      "metadata": {
        "id": "fcm_GIsc3P9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the resulting classifier\n",
        "h = 0.02\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "Z = np.dot(np.c_[xx.ravel(), yy.ravel()], W) + b\n",
        "Z = np.argmax(Z, axis=1)\n",
        "Z = Z.reshape(xx.shape)\n",
        "fig = plt.figure()\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "#fig.savefig('spiral_linear.png')\n"
      ],
      "metadata": {
        "id": "Z6i_ZYuK2h7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<a name='net'></a>\n",
        "\n",
        "## 3. ニューラルネットワークによる訓練\n",
        "<!-- ## Training a Neural Network -->\n",
        "\n",
        "明らかに，線形分類器はこのデータセットには不適当であり，我々はニューラルネットワークを使いたいと考えています。\n",
        "このおもちゃデータでは隠れ層を 1 つ追加すれば十分です。\n",
        "ここで，重みとバイアスを 2 セット (第 1 層と第 2 層) 必要とします。\n",
        "<!-- Clearly, a linear classifier is inadequate for this dataset and we would like to use a Neural Network. \n",
        "One additional hidden layer will suffice for this toy data. \n",
        "We will now need two sets of weights and biases (for the first and second layers): -->\n"
      ],
      "metadata": {
        "id": "j3Wh_7pj17oC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# パラメータを乱数で初期化する\n",
        "h = 100             # 中間層のニューロン数\n",
        "W = 0.01 * np.random.randn(D,h)\n",
        "b = np.zeros((1,h))\n",
        "W2 = 0.01 * np.random.randn(h,K)\n",
        "b2 = np.zeros((1,K))"
      ],
      "metadata": {
        "id": "zxU_fAtJ3tVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "得点を計算する順向きパスの形が変わりました。\n",
        "<!-- The forward pass to compute scores now changes form: -->\n"
      ],
      "metadata": {
        "id": "ELEyfnSl3raA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate class scores with a 2-layer Neural Network\n",
        "hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n",
        "scores = np.dot(hidden_layer, W2) + b2"
      ],
      "metadata": {
        "id": "hYhCfDhH4VGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以前からの唯一の変更は 1 行の追加コードで，最初に隠れ層の表現を計算し，次にこの隠れ層に基づく得点を計算していることに注意してください。\n",
        "重要なのは，非線形性を追加したことです。\n",
        "この場合は単純な ReLU で，隠れ層の活性度をゼロで閾値付けします。\n",
        "<!-- Notice that the only change from before is one extra line of code, where we first compute the hidden layer representation and then the scores based on this hidden layer. \n",
        "Crucially, we've also added a non-linearity, which in this case is simple ReLU that thresholds the activations on the hidden layer at zero.-->\n",
        "\n",
        "その他はすべて同じです。\n",
        "前と全く同じように得点に基づいて損失を計算し，前と全く同じように得点 `dscores` の勾配を求めます。\n",
        "しかし，その勾配をモデルパラメータに誤差逆伝播する方法は，当然ながら形を変えます。まず，ニューラルネットワークの 2 層目を誤差逆伝播してみましょう。\n",
        "これはソフトマックス分類器のコードと同じですが，変数 `X` (生データ) を `hidden_layer`) に置き換えています。\n",
        "<!-- Everything else remains the same. \n",
        "We compute the loss based on the scores exactly as before, and get the gradient for the scores `dscores` exactly as before. \n",
        "However, the way we backpropagate that gradient into the model parameters now changes form, of course. First lets backpropagate the second layer of the Neural Network. \n",
        "This looks identical to the code we had for the Softmax classifier, except we're replacing `X` (the raw data), with the variable `hidden_layer`): -->\n"
      ],
      "metadata": {
        "id": "xGi7WhDY4Rss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# backpropate the gradient to the parameters\n",
        "# first backprop into parameters W2 and b2\n",
        "dW2 = np.dot(hidden_layer.T, dscores)\n",
        "db2 = np.sum(dscores, axis=0, keepdims=True)"
      ],
      "metadata": {
        "id": "iiNLo0zF5F7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "なぜなら `hidden_layer` 自身が他のパラメータとデータの関数だからです。\n",
        "でも，先ほどとは異なり，まだ終わっていません。\n",
        "この変数を通して誤差逆伝播を継続する必要があります。\n",
        "その勾配は次のように計算されます。\n",
        "<!-- However, unlike before we are not yet done, because `hidden_layer` is itself a function of other parameters and the data! We need to continue backpropagation through this variable. \n",
        "Its gradient can be computed as: -->\n",
        "\n"
      ],
      "metadata": {
        "id": "kZIcx2l35EVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dhidden = np.dot(dscores, W2.T)"
      ],
      "metadata": {
        "id": "zOamxFA-5cL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "これで隠れ層の出力に勾配がつきました。\n",
        "次に ReLU の非線形性を誤差逆伝播する必要があります。\n",
        "これは簡単なことで，逆向きパスの ReLU は事実上スイッチであることがわかります。\n",
        "$r=\\max(0, x)$ なので $\\frac{dr}{dx}=1(x > 0)$ となることがわかります。\n",
        "合成関数の微分法則と組み合わせて ReLU ユニットは，入力が 0 より大きかったら勾配を変更せずに通過させます。\n",
        "ですが，前向きパスの間に入力が 0 より小さかったら **抹消**  されることがわかります。\n",
        "従って ReLU をそのまま誤差逆伝播することは，次のように簡単にできます。\n",
        "<!-- Now we have the gradient on the outputs of the hidden layer. \n",
        "Next, we have to backpropagate the ReLU non-linearity. \n",
        "This turns out to be easy because ReLU during the backward pass is effectively a switch. \n",
        "Since $r = max(0, x)$, we have that $\\frac{dr}{dx} = 1(x > 0)$. \n",
        "Combined with the chain rule, we see that the ReLU unit lets the gradient pass through unchanged if its input was greater than 0, but *kills it* if its input was less than zero during the forward pass. \n",
        "Hence, we can backpropagate the ReLU in place simply with: -->\n"
      ],
      "metadata": {
        "id": "2pZLQ0OF5Yss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# backprop the ReLU non-linearity\n",
        "dhidden[hidden_layer <= 0] = 0"
      ],
      "metadata": {
        "id": "1euu4xh16KnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "そして、いよいよ 1 層目の重みと偏りに続きます。\n"
      ],
      "metadata": {
        "id": "axCA_9t86R6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finally into W,b\n",
        "dW = np.dot(X.T, dhidden)\n",
        "db = np.sum(dhidden, axis=0, keepdims=True)\n"
      ],
      "metadata": {
        "id": "2DsCeT2Z6UU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "これで完了です。\n",
        "勾配 `dW,db,dW2,db2` を取得し，パラメータの更新を実行することができます。\n",
        "他のすべては変更されないままです。\n",
        "完全なコードは非常によく似ています。\n",
        "<!-- We're done! \n",
        "We have the gradients `dW,db,dW2,db2` and can perform the parameter update. \n",
        "Everything else remains unchanged. The full code looks very similar: -->\n"
      ],
      "metadata": {
        "id": "1OOdgX5k6WtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize parameters randomly\n",
        "h = 100 # size of hidden layer\n",
        "W = 0.01 * np.random.randn(D,h)\n",
        "b = np.zeros((1,h))\n",
        "W2 = 0.01 * np.random.randn(h,K)\n",
        "b2 = np.zeros((1,K))\n",
        "\n",
        "# some hyperparameters\n",
        "step_size = 1e-0\n",
        "_lambda = 1e-3 # regularization strength\n",
        "\n",
        "# gradient descent loop\n",
        "num_examples = X.shape[0]\n",
        "for i in range(10000):\n",
        "  \n",
        "    # evaluate class scores, [N x K]\n",
        "    hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n",
        "    scores = np.dot(hidden_layer, W2) + b2\n",
        "  \n",
        "    # compute the class probabilities\n",
        "    exp_scores = np.exp(scores)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
        "  \n",
        "    # compute the loss: average cross-entropy loss and regularization\n",
        "    corect_logprobs = -np.log(probs[range(num_examples),y])\n",
        "    data_loss = np.sum(corect_logprobs)/num_examples\n",
        "    reg_loss = 0.5 * _lambda * np.sum(W*W) + 0.5 * _lambda * np.sum(W2*W2)\n",
        "    loss = data_loss + reg_loss\n",
        "    if i % 1000 == 0:\n",
        "        print(f\"繰り返し回数:{i:4d} 損失値: {loss:.3f}\")\n",
        "  \n",
        "    # compute the gradient on scores\n",
        "    dscores = probs\n",
        "    dscores[range(num_examples),y] -= 1\n",
        "    dscores /= num_examples\n",
        "  \n",
        "    # backpropate the gradient to the parameters\n",
        "    # first backprop into parameters W2 and b2\n",
        "    dW2 = np.dot(hidden_layer.T, dscores)\n",
        "    db2 = np.sum(dscores, axis=0, keepdims=True)\n",
        "    # next backprop into hidden layer\n",
        "    dhidden = np.dot(dscores, W2.T)\n",
        "    # backprop the ReLU non-linearity\n",
        "    dhidden[hidden_layer <= 0] = 0\n",
        "    # finally into W,b\n",
        "    dW = np.dot(X.T, dhidden)\n",
        "    db = np.sum(dhidden, axis=0, keepdims=True)\n",
        "  \n",
        "    # add regularization gradient contribution\n",
        "    dW2 += _lambda * W2\n",
        "    dW += _lambda * W\n",
        "  \n",
        "    # perform a parameter update\n",
        "    W += -step_size * dW\n",
        "    b += -step_size * db\n",
        "    W2 += -step_size * dW2\n",
        "    b2 += -step_size * db2\n"
      ],
      "metadata": {
        "id": "3YP99XuQ6ngn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "出力例は以下のようになります\n",
        "\n",
        "```\n",
        "繰り返し回数:   0 損失値: 1.099\n",
        "繰り返し回数:1000 損失値: 0.288\n",
        "繰り返し回数:2000 損失値: 0.254\n",
        "繰り返し回数:3000 損失値: 0.249\n",
        "繰り返し回数:4000 損失値: 0.247\n",
        "繰り返し回数:5000 損失値: 0.246\n",
        "繰り返し回数:6000 損失値: 0.245\n",
        "繰り返し回数:7000 損失値: 0.245\n",
        "繰り返し回数:8000 損失値: 0.245\n",
        "繰り返し回数:9000 損失値: 0.245\n",
        "```"
      ],
      "metadata": {
        "id": "ye3ogtr17Sca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "訓練精度を次のように計算されます:\n"
      ],
      "metadata": {
        "id": "LftsyidU7gLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate training set accuracy\n",
        "hidden_layer = np.maximum(0, np.dot(X, W) + b)\n",
        "scores = np.dot(hidden_layer, W2) + b2\n",
        "predicted_class = np.argmax(scores, axis=1)\n",
        "print(f'訓練精度: {(np.mean(predicted_class == y)):.2f}')"
      ],
      "metadata": {
        "id": "U3yIAK_U7mj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**99%**と印刷されます。\n",
        "判定境界を可視化することもできます。\n",
        "<!-- Which prints **98%**!. \n",
        "We can also visualize the decision boundaries: -->\n"
      ],
      "metadata": {
        "id": "LgOeG7CO731x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the resulting classifier\n",
        "h = 0.02\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "Z = np.dot(np.maximum(0, np.dot(np.c_[xx.ravel(), yy.ravel()], W) + b), W2) + b2\n",
        "Z = np.argmax(Z, axis=1)\n",
        "Z = Z.reshape(xx.shape)\n",
        "fig = plt.figure()\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "#fig.savefig('spiral_net.png')"
      ],
      "metadata": {
        "id": "KYP-9r2N8Hot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. まとめ\n",
        "\n",
        "おもちゃの 2 次元データセットを使って，線形ネットワークと 2 層ニューラルネットワークの両方を学習させました。\n",
        "線形分類器からニューラルネットワークへの変更には，コードの変更がほとんどないことがわかりました。\n",
        "得点関数の形が変わり (1 行のコードの違い)，誤差逆伝播の形が変わります (隠れ層からネットワークの第 1 層への誤差逆伝播をもう 1 ラウンド実行する必要があります)。\n",
        "<!-- We've worked with a toy 2D dataset and trained both a linear network and a 2-layer Neural Network. \n",
        "We saw that the change from a linear classifier to a Neural Network involves very few changes in the code. \n",
        "The score function changes its form (1 line of code difference), and the backpropagation changes its form (we have to perform one more round of backprop through the hidden layer to the first layer of the network). -->\n",
        "<!-- \n",
        "- You may want to look at this IPython Notebook code [rendered as HTML](http://cs.stanford.edu/people/karpathy/cs231nfiles/minimal_net.html).\n",
        "- Or download the [ipynb file](http://cs.stanford.edu/people/karpathy/cs231nfiles/minimal_net.ipynb) -->\n",
        "\n"
      ],
      "metadata": {
        "id": "GlkA-jbj6JIc"
      }
    }
  ]
}